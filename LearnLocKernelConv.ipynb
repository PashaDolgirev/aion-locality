{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75ad941b",
   "metadata": {},
   "source": [
    "Total energy now contains local density-density interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8622f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import math, os, time, copy\n",
    "import torch.fft as tfft\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "torch.random.manual_seed(1234) # for reproducibility\n",
    "\n",
    "# Global settings\n",
    "dtype = torch.float64\n",
    "device = \"cpu\"\n",
    "\n",
    "data_regime = \"smooth\" # \"smooth\" or \"rough\"\n",
    "\n",
    "N_grid = 512 # number of grid points\n",
    "if data_regime == \"smooth\":\n",
    "    M_cutoff = 50 # maximum harmonic\n",
    "    m = torch.arange(1, M_cutoff+1, dtype=dtype, device=device)             # (M,)\n",
    "    x = torch.linspace(0, 1, N_grid, dtype=dtype, device=device)            # (N,)\n",
    "    #design matrix needed to sample densities\n",
    "    DesignMatrix = torch.cos(torch.pi * torch.outer(m, x))                  # (M, N)\n",
    "    DerDM = -torch.pi * m[:, None] * torch.sin(torch.pi * torch.outer(m, x))  # (M, N) # derivative of design matrix\n",
    "    std_harm = 2.0 / (1.0 + m)**2\n",
    "elif data_regime == \"rough\":\n",
    "    M_cutoff = N_grid # maximum harmonic\n",
    "    m = torch.arange(1, M_cutoff+1, dtype=dtype, device=device)             # (M,)\n",
    "    x = torch.linspace(0, 1, N_grid, dtype=dtype, device=device)            # (N,)\n",
    "    #design matrix needed to sample densities\n",
    "    DesignMatrix = torch.cos(torch.pi * torch.outer(m, x))                  # (M, N)\n",
    "    DerDM = -torch.pi * m[:, None] * torch.sin(torch.pi * torch.outer(m, x))  # (M, N) # derivative of design matrix\n",
    "    std_harm = 2.0 / (1.0 + 0.0 * m)**2\n",
    "else:\n",
    "    raise ValueError(\"regime must be 'smooth' or 'rough'\")\n",
    "\n",
    "N_train = 1500\n",
    "N_test = 250\n",
    "N_val = 250\n",
    "\n",
    "N_batch = 50\n",
    "N_epochs = 10000\n",
    "lr = 1e-3 # we will use a LR scheduler, so this is just an initial value\n",
    "min_delta = 1e-5 # min change in the monitored quantity to qualify as an improvement\n",
    "patience = 30    # epochs to wait for improvement before stopping training\n",
    "\n",
    "\n",
    "kernel_regime = \"double_Gaussian\" # \"single_Gaussian\" or \"double_Gaussian\" \n",
    "if kernel_regime == \"single_Gaussian\":\n",
    "    amp_Gaussian = 1.0 # amplitude of Gaussian kernel for interaction energy\n",
    "    sigma_Gaussian = 3.0 # width of Gaussian kernel for interaction energy\n",
    "elif kernel_regime == \"double_Gaussian\":\n",
    "    amp_Gaussian_1 = -1.0 # amplitude of first Gaussian kernel for interaction energy\n",
    "    sigma_Gaussian_1 = 3.0 # width of first Gaussian kernel for interaction energy\n",
    "    amp_Gaussian_2 = 2.0 # amplitude of second Gaussian kernel for interaction energy\n",
    "    sigma_Gaussian_2 = 1.0 # width of second Gaussian kernel for interaction energy\n",
    "else:\n",
    "    raise ValueError(\"Here kernel_regime must be 'single_Gaussian' or 'double_Gaussian'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864ea1c4",
   "metadata": {},
   "source": [
    "Setup the functional and sample density profiles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9003c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_density(*, rho_b=0.0):\n",
    "    \"\"\"\n",
    "    Sample rho_j = rho_avg + sum_{m=1}^M a_m cos(m pi x_j), with x_j in [0,1]\n",
    "    this sampling of amplitudes a_m implies that the derivatives at the boundaries are zero\n",
    "    Sampling is done such that the generated density has zero mean over the grid points\n",
    "    Returns:\n",
    "      rho : (N,) density profile\n",
    "    \"\"\"    \n",
    "    a = torch.normal(torch.zeros_like(std_harm), std_harm)\n",
    "    rho = a @ DesignMatrix\n",
    "    d_rho_a = a @ DerDM  # derivative of rho w.r.t. x\n",
    "\n",
    "    rho = rho - 0.0 * rho.mean()\n",
    "\n",
    "    return rho, d_rho_a, a\n",
    "\n",
    "def sample_density_batch(B: int, rho_b=0.0):\n",
    "    \"\"\"\n",
    "    Sample a batch of B density profiles\n",
    "    Spatial average density rho_avg is set to zero\n",
    "    Returns rho: (B, N_grid)\n",
    "    \"\"\"\n",
    "\n",
    "    a = torch.normal(torch.zeros(B, std_harm.numel(),dtype=dtype, device=device), std_harm.expand(B, -1))\n",
    "    rho = a @ DesignMatrix  # (B, N_grid)\n",
    "    d_rho_a = a @ DerDM  # (B, N_grid) # derivative of rho w.r.t. x\n",
    "\n",
    "    rho = rho - 0.0 * rho.mean(dim=1, keepdim=True)\n",
    "    return rho, d_rho_a, a\n",
    "\n",
    "# density–density interaction kernels K(r)\n",
    "# r: tensor (can be negative)\n",
    "def K_gaussian(r, sigma=1.0):       # strictly local-ish\n",
    "    r = r.to(dtype=dtype)\n",
    "    return torch.exp(-(r**2) / (sigma**2))\n",
    "\n",
    "def K_exp(r, xi=2.0):              # short–to–intermediate range\n",
    "    r = r.to(dtype=dtype)\n",
    "    return torch.exp(-torch.abs(r) / xi)\n",
    "\n",
    "def K_yukawa(r, lam=10.0):         # long-range but screened\n",
    "    r = torch.abs(r).to(dtype=dtype)\n",
    "    r = r.clamp(min=1.0)         # avoid r=0 singularity\n",
    "    return torch.exp(-r / lam) / r\n",
    "\n",
    "def K_power(r, alpha=1.0):         # unscreened long range (Coulomb-like for alpha=1)\n",
    "    r = torch.abs(r).to(dtype=dtype)\n",
    "    r = r.clamp(min=1.0)\n",
    "    return 1.0 / (r**alpha)\n",
    "\n",
    "def E_int_conv(rho: torch.Tensor, kernel: str, **kwargs) -> torch.Tensor: \n",
    "    \"\"\"\n",
    "    Interaction energy using convolution\n",
    "    rho: (N,) or (B, N)\n",
    "    kernel: \"gaussian\", \"exp\", \"yukawa\", \"power\"\n",
    "    kwargs: parameters for the kernel function (sigma, xi, lam, alpha, etc.)\n",
    "    Returns: scalar (if input 1D) or (B,) (if input 2D)\n",
    "    \"\"\"\n",
    "    if kernel == \"gaussian\":\n",
    "        K_fun = K_gaussian\n",
    "    elif kernel == \"exp\":\n",
    "        K_fun = K_exp\n",
    "    elif kernel == \"yukawa\":\n",
    "        K_fun = K_yukawa\n",
    "    elif kernel == \"power\":\n",
    "        K_fun = K_power\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown kernel: {kernel}\")\n",
    "\n",
    "    # ensure batch dim\n",
    "    if rho.dim() == 1:\n",
    "        rho = rho.unsqueeze(0)\n",
    "    B, N = rho.shape\n",
    "    device, dtype = rho.device, rho.dtype\n",
    "\n",
    "    # r = -(N-1)..(N-1), kernel length = 2N-1\n",
    "    r_vals = torch.arange(-(N-1), N, device=device, dtype=dtype)  # (2N-1,)\n",
    "    k_full = K_fun(r_vals, **kwargs)                              # (2N-1,)\n",
    "\n",
    "    weight = k_full.view(1, 1, -1)                # (1,1,2N-1)\n",
    "    u = F.conv1d(rho.unsqueeze(1), weight, padding=N-1).squeeze(1) # (B, N)\n",
    "    E = 0.5 * (rho * u).sum(dim=-1) / N  # (B,)\n",
    "    return E.squeeze(0) if E.numel() == 1 else E\n",
    "\n",
    "\n",
    "if kernel_regime == \"single_Gaussian\":\n",
    "    def E_tot(rho: torch.Tensor) -> torch.Tensor:\n",
    "        return amp_Gaussian * E_int_conv(rho, kernel=\"gaussian\", sigma=sigma_Gaussian)\n",
    "elif kernel_regime == \"double_Gaussian\":\n",
    "    def E_tot(rho: torch.Tensor) -> torch.Tensor:\n",
    "        return (amp_Gaussian_1 * E_int_conv(rho, kernel=\"gaussian\", sigma=sigma_Gaussian_1) +\n",
    "                amp_Gaussian_2 * E_int_conv(rho, kernel=\"gaussian\", sigma=sigma_Gaussian_2))\n",
    "\n",
    "\n",
    "R = 10 \n",
    "r_grid = torch.arange(-R, R+1)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "if kernel_regime == \"single_Gaussian\":\n",
    "    plt.plot(r_grid, amp_Gaussian * K_gaussian(r_grid, sigma=sigma_Gaussian), 'o-', linewidth=2)\n",
    "elif kernel_regime == \"double_Gaussian\":\n",
    "    plt.plot(r_grid, amp_Gaussian_1 * K_gaussian(r_grid, sigma=sigma_Gaussian_1) + amp_Gaussian_2 * K_gaussian(r_grid, sigma=sigma_Gaussian_2), 'o-', linewidth=2)\n",
    "plt.axhline(0, color='k', linewidth=0.5)\n",
    "plt.xlabel(\"r = i - j\")\n",
    "plt.ylabel(\"K_r\")\n",
    "plt.title(f\"Local kernel\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "rho_batch, _, _ = sample_density_batch(3)  # (B, N_grid)\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(x.numpy(), rho_batch[0, :].numpy(), lw=2)\n",
    "plt.plot(x.numpy(), rho_batch[1, :].numpy(), lw=2)\n",
    "plt.plot(x.numpy(), rho_batch[2, :].numpy(), lw=2)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"rho(x)\")\n",
    "plt.title(\"Random sampled densities from cosine basis\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddb6d68",
   "metadata": {},
   "source": [
    "Feature processing (generation and normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e5f54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we save features as (B, N_grid, N_feat), where N_feat is the number of features per grid point\n",
    "# generate train/test split\n",
    "\n",
    "def compute_normalization_stats(features):\n",
    "    \"\"\"\n",
    "    Compute mean and std for features with shape (N_data, N_grid, N_feat)\n",
    "    Averages over both data and spatial dimensions\n",
    "    \n",
    "    Args:\n",
    "        features: torch.Tensor of shape (N_data, N_grid, N_feat)\n",
    "    \n",
    "    Returns:\n",
    "        mean: torch.Tensor of shape (1, 1, N_feat)\n",
    "        std: torch.Tensor of shape (1, 1, N_feat)\n",
    "    \"\"\"\n",
    "\n",
    "    mean_feat = features.mean(dim=(0, 1), keepdim=True)  # Shape: (1, 1, N_feat)\n",
    "    std_feat = features.std(dim=(0, 1), keepdim=True) # Shape: (1, 1, N_feat)\n",
    "    \n",
    "    return mean_feat, std_feat\n",
    "\n",
    "def normalize_features(features, mean_feat, std_feat):\n",
    "    \"\"\"\n",
    "    Normalize features using provided or computed statistics\n",
    "    \n",
    "    Args:\n",
    "        features: torch.Tensor of shape (B, N_grid, N_feat)\n",
    "        mean: torch.Tensor of shape (1, 1, N_feat)\n",
    "        std: torch.Tensor of shape (1, 1, N_feat)\n",
    "\n",
    "    Returns:\n",
    "        normalized_features: torch.Tensor of same shape as input\n",
    "        mean: mean used for normalization\n",
    "        std: std used for normalization\n",
    "    \"\"\"\n",
    "    normalized_features = (features - mean_feat) / std_feat\n",
    "\n",
    "    return normalized_features\n",
    "\n",
    "def generate_loc_features_rs(rho: torch.Tensor, N_feat=2) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Generate local features from density rho\n",
    "    rs, real space   \n",
    "    Args:\n",
    "        rho: torch.Tensor of shape (B, N_grid)\n",
    "        N_feat: int, number of features to generate\n",
    "\n",
    "    Returns:\n",
    "        features: torch.Tensor of shape (B, N_grid, N_feat)\n",
    "        each feature is of the form rho^k, k=1,...,N_feat\n",
    "    \"\"\"\n",
    "    features = [rho.unsqueeze(-1) ** k for k in range(1, N_feat + 1)]\n",
    "    return torch.cat(features, dim=-1)\n",
    "\n",
    "def generate_loc_features_ms(d_rho: torch.Tensor, N_feat=2) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Generate local features from density derivative d_rho\n",
    "    ms, momentum space\n",
    "    Args:\n",
    "        d_rho: torch.Tensor of shape (B, N_grid)\n",
    "        N_feat: int, number of features to generate\n",
    "\n",
    "    Returns:\n",
    "        features: torch.Tensor of shape (B, N_grid, N_feat)\n",
    "        each feature is of the form d_rho^k, k=1,...,N_feat\n",
    "    \"\"\"\n",
    "    features = [d_rho.unsqueeze(-1) ** k for k in range(1, N_feat + 1)]\n",
    "    return torch.cat(features, dim=-1)\n",
    "\n",
    "rho_train, d_rho_train, a_train = sample_density_batch(N_train)  # (N_train, N_grid)\n",
    "rho_test, d_rho_test, a_test = sample_density_batch(N_test)   # (N_test, N_grid)\n",
    "rho_val, d_rho_val, a_val = sample_density_batch(N_val)    # (N_val, N_grid)\n",
    "\n",
    "N_feat = 1 \n",
    "\n",
    "features_train_rs = generate_loc_features_rs(rho_train, N_feat=N_feat)  # (N_train, N_grid, N_feat)\n",
    "features_test_rs  = generate_loc_features_rs(rho_test, N_feat=N_feat)   # (N_test, N_grid, N_feat)\n",
    "features_val_rs   = generate_loc_features_rs(rho_val, N_feat=N_feat)    # (N_val, N_grid, N_feat)\n",
    "\n",
    "features_train_ms = generate_loc_features_ms(d_rho_train, N_feat=N_feat)  # (N_train, N_grid, N_feat)\n",
    "features_test_ms  = generate_loc_features_ms(d_rho_test, N_feat=N_feat)   # (N_test, N_grid, N_feat)\n",
    "features_val_ms   = generate_loc_features_ms(d_rho_val, N_feat=N_feat)    # (N_val, N_grid, N_feat)\n",
    "\n",
    "features_train = torch.cat([features_train_rs, features_train_ms], dim=-1)\n",
    "features_test  = torch.cat([features_test_rs, features_test_ms], dim=-1)\n",
    "features_val   = torch.cat([features_val_rs, features_val_ms], dim=-1)\n",
    "\n",
    "targets_train = E_tot(rho_train)            # (N_train,)\n",
    "targets_test  = E_tot(rho_test)             # (N_test,)\n",
    "targets_val   = E_tot(rho_val)              # (N_val,)\n",
    "\n",
    "# Normalize features\n",
    "mean_feat, std_feat = compute_normalization_stats(features_train)\n",
    "features_train_norm = normalize_features(features_train, mean_feat, std_feat)\n",
    "features_test_norm = normalize_features(features_test, mean_feat, std_feat)\n",
    "features_val_norm = normalize_features(features_val, mean_feat, std_feat)\n",
    "\n",
    "# Normalize targets\n",
    "E_mean = targets_train.mean()\n",
    "E_std = targets_train.std()\n",
    "targets_train_norm = (targets_train - E_mean) / E_std\n",
    "targets_test_norm = (targets_test - E_mean) / E_std\n",
    "targets_val_norm = (targets_val - E_mean) / E_std\n",
    "\n",
    "# Datasets\n",
    "train_dataset = TensorDataset(features_train_norm, targets_train_norm)\n",
    "val_dataset   = TensorDataset(features_val_norm,   targets_val_norm)\n",
    "test_dataset  = TensorDataset(features_test_norm,  targets_test_norm)\n",
    "\n",
    "# Loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=N_batch, shuffle=True,  drop_last=False)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=N_batch, shuffle=False, drop_last=False)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=N_batch, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9eed5b",
   "metadata": {},
   "source": [
    "Correlation function analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d40013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analytical routines\n",
    "\n",
    "def C_mm(m_vals: torch.Tensor, r_vals: torch.Tensor, N: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute C_mm(r) for a list of m-values and r-values.\n",
    "    Handles the singular mode m = N-1 cleanly.\n",
    "    \n",
    "    Returns: (M, R) tensor\n",
    "    \"\"\"\n",
    "    dtype = m_vals.dtype\n",
    "    device = m_vals.device\n",
    "\n",
    "    m = m_vals.view(-1, 1)              # (M,1)\n",
    "    r = torch.abs(r_vals).view(1, -1)   # (1,R)\n",
    "\n",
    "    pi = torch.tensor(math.pi, dtype=dtype, device=device)\n",
    "    theta = pi * m / (N - 1)            # (M,1)\n",
    "\n",
    "    # First term: (1/2) cos(theta*r)\n",
    "    term1 = 0.5 * torch.cos(theta * r)\n",
    "\n",
    "    # denominator sin(theta)\n",
    "    den = torch.sin(theta)              # (M,1)\n",
    "\n",
    "    # numerator sin(theta * (N-r))\n",
    "    num = torch.sin(theta * (N - r))    # (M,R)\n",
    "\n",
    "    # (-1)^m \n",
    "    sign = (m_vals.to(torch.long) % 2)*(-2) + 1  \n",
    "    # maps even -> 1, odd -> -1\n",
    "    sign = sign.view(-1,1).to(dtype)\n",
    "\n",
    "    # general closed-form (will have 0/0 for m=N-1)\n",
    "    term2 = 0.5 * sign * (num / den) / (N - r)\n",
    "\n",
    "    # m = N-1  (exact expression = (-1)^r)\n",
    "    mask_sing = (m_vals == (N-1))\n",
    "    if mask_sing.any():\n",
    "        idx = torch.nonzero(mask_sing, as_tuple=False).view(-1)\n",
    "        C_sing = torch.cos(pi * r)          # (-1)^r\n",
    "        term2[idx, :] = C_sing - term1[idx, :]   # enforce full C = term1+term2 ⇒ C=cos(pi*r)\n",
    "\n",
    "    C = term1 + term2\n",
    "    return C\n",
    "\n",
    "def C_m1m2_sym(m_vals: torch.Tensor, r_vals: torch.Tensor, N: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute the symmetrized correlator C_{m1,m2}(r) for all m1,m2 and r.\n",
    "\n",
    "    Formula:\n",
    "        C_{m1, m2}(r) =\n",
    "            [cos(pi (m1 - m2)/2) / (2 (N - |r|))] *\n",
    "            [ sin(pi (m1 - m2)(N - |r|)/(2 (N-1))) *\n",
    "              cos(pi (m1 + m2)|r|/(2 (N-1))) ] /\n",
    "            sin(pi (m1 - m2)/(2 (N-1)))\n",
    "        +\n",
    "            [cos(pi (m1 + m2)/2) / (2 (N - |r|))] *\n",
    "            [ sin(pi (m1 + m2)(N - |r|)/(2 (N-1))) *\n",
    "              cos(pi (m1 - m2)|r|/(2 (N-1))) ] /\n",
    "            sin(pi (m1 + m2)/(2 (N-1)))\n",
    "\n",
    "    Might require more care for singular cases where the denominators vanish.\n",
    "\n",
    "    Returns:\n",
    "        C: (M, M, R) tensor where C[m1_idx, m2_idx, r_idx] = C_{m1,m2}(r)\n",
    "    \"\"\"\n",
    "    dtype = m_vals.dtype\n",
    "    device = m_vals.device\n",
    "    M = m_vals.numel()\n",
    "\n",
    "    # Shapes: (M,1,1) and (1,M,1)\n",
    "    m1 = m_vals.view(-1, 1, 1)    # (M,1,1)\n",
    "    m2 = m_vals.view(1, -1, 1)    # (1,M,1)\n",
    "\n",
    "    # |r| as (1,1,R)\n",
    "    r_abs = torch.abs(r_vals).view(1, 1, -1).to(device)  # (1,1,R)\n",
    "\n",
    "    pi = torch.tensor(math.pi, dtype=dtype, device=device)\n",
    "\n",
    "    # Convenient combos\n",
    "    d = m1 - m2  # (M,M,1)\n",
    "    s = m1 + m2  # (M,M,1)\n",
    "\n",
    "    # N - |r| as (1,1,R)\n",
    "    L = N - r_abs\n",
    "\n",
    "    # Common denominators: alpha_d = pi d / [2 (N-1)], alpha_s = pi s / [2 (N-1)]\n",
    "    alpha_d = pi * d / (2 * (N - 1))  # (M,M,1)\n",
    "    alpha_s = pi * s / (2 * (N - 1))  # (M,M,1)\n",
    "\n",
    "    # Denominators\n",
    "    den_d = torch.sin(alpha_d)       # (M,M,1)\n",
    "    den_s = torch.sin(alpha_s)       # (M,M,1)\n",
    "\n",
    "    # Small epsilon to avoid division by exact 0\n",
    "    eps = 1e-12\n",
    "    den_d_safe = den_d.clone()\n",
    "    den_s_safe = den_s.clone()\n",
    "    den_d_safe[den_d_safe.abs() < eps] = eps\n",
    "    den_s_safe[den_s_safe.abs() < eps] = eps\n",
    "\n",
    "    # Numerators for the two terms\n",
    "    num_d = torch.sin(pi * d * L / (2 * (N - 1))) * \\\n",
    "            torch.cos(pi * s * r_abs / (2 * (N - 1)))  # (M,M,R)\n",
    "\n",
    "    num_s = torch.sin(pi * s * L / (2 * (N - 1))) * \\\n",
    "            torch.cos(pi * d * r_abs / (2 * (N - 1)))  # (M,M,R)\n",
    "\n",
    "    # Prefactors cos(pi (m1±m2)/2) / [2 (N - |r|)]\n",
    "    pref_d = torch.cos(pi * d / 2.0) / (2.0 * L)   # (M,M,R)\n",
    "    pref_s = torch.cos(pi * s / 2.0) / (2.0 * L)   # (M,M,R)\n",
    "\n",
    "    termA = pref_d * (num_d / den_d_safe)          # (M,M,R)\n",
    "    termB = pref_s * (num_s / den_s_safe)          # (M,M,R)\n",
    "\n",
    "    C = termA + termB  # (M,M,R), general off-diagonal expression\n",
    "\n",
    "    # --- Diagonal correction: m1 == m2 ---\n",
    "    # For diagonal we use the exact C_mm(r), including the m = N-1 singular mode\n",
    "    C_diag = C_mm(m_vals, r_vals, N)   # (M,R)\n",
    "\n",
    "    idx = torch.arange(M, device=device)\n",
    "    C[idx, idx, :] = C_diag            # overwrite diagonal slices\n",
    "\n",
    "    return C\n",
    "\n",
    "\n",
    "def C_m1m2_sym_def(m_vals: torch.Tensor, r_vals: torch.Tensor, N: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute the symmetrized correlator C_{m1,m2}(r) from the *definition*:\n",
    "\n",
    "        x_j = (j)/(N-1), j = 0,...,N-1\n",
    "        phi_m(j) = cos(pi m x_j)\n",
    "\n",
    "        tilde C_{m1,m2}(r) = 1/(N-|r|) sum_{i=0}^{N-|r|-1}\n",
    "                             phi_{m1}(i) phi_{m2}(i+|r|)\n",
    "        C_{m1,m2}(r) = 0.5[ tilde C_{m1,m2}(r) + tilde C_{m2,m1}(r) ]\n",
    "\n",
    "    Args:\n",
    "        m_vals: (M,) tensor of modes m >= 1\n",
    "        r_vals: (R,) tensor of integer r (can be positive or negative)\n",
    "        N:      number of grid points\n",
    "\n",
    "    Returns:\n",
    "        C: (M, M, R) tensor with C[m1_idx, m2_idx, r_idx] = C_{m1,m2}(r)\n",
    "    \"\"\"\n",
    "    dtype = m_vals.dtype\n",
    "    device = m_vals.device\n",
    "\n",
    "    m_vals = m_vals.to(dtype=dtype, device=device)\n",
    "    r_vals = r_vals.to(device=device)\n",
    "\n",
    "    M = m_vals.numel()\n",
    "    Rlen = r_vals.numel()\n",
    "\n",
    "    # grid x_j = j/(N-1), j = 0,...,N-1 (this matches x_j = (j-1)/(N-1) with a shift)\n",
    "    j = torch.arange(0, N, device=device, dtype=dtype)       # (N,)\n",
    "    x = j / (N - 1)                                          # (N,)\n",
    "\n",
    "    # phi[m, j] = cos(pi m x_j)\n",
    "    pi = torch.tensor(math.pi, dtype=dtype, device=device)\n",
    "    phi = torch.cos(pi * m_vals.view(M, 1) * x.view(1, N))   # (M, N)\n",
    "\n",
    "    # Output tensor\n",
    "    C = torch.empty((M, M, Rlen), dtype=dtype, device=device)\n",
    "\n",
    "    for idx, r in enumerate(r_vals):\n",
    "        k = abs(int(r.item()))\n",
    "        L = N - k  # number of terms in sum\n",
    "\n",
    "        # segments phi(:, i) and phi(:, i+k)\n",
    "        A = phi[:, :L]          # (M, L)\n",
    "        B = phi[:, k:k+L]       # (M, L)\n",
    "\n",
    "        # tilde_C[m1, m2](r) = 1/L sum_i A[m1,i] * B[m2,i]\n",
    "        # => (M,L) @ (M,L)^T with appropriate transpose:\n",
    "        # we want A * B for (m1,m2) so:\n",
    "        tilde_C = (A @ B.t()) / L    # (M, M)\n",
    "\n",
    "        # symmetrize: C = 0.5(tilde_C + tilde_C^T)\n",
    "        C_sym = 0.5 * (tilde_C + tilde_C.T)  # (M, M)\n",
    "\n",
    "        C[:, :, idx] = C_sym\n",
    "\n",
    "    return C\n",
    "\n",
    "def second_moment_analytical(R: int, N: int) -> float:\n",
    "    \"\"\"\n",
    "    Second moment of the density–density correlation function,\n",
    "    computed analytically from the harmonic expansion.\n",
    "    \"\"\"\n",
    "\n",
    "    gamma2 = std_harm**2      # (M,)\n",
    "    r_vals = torch.arange(-R, R+1, device=device)   # (2R+1,)\n",
    "    \n",
    "    C = (gamma2.unsqueeze(1) * C_mm(m, r_vals, N_grid)).sum(dim=0) # (2R+1,)\n",
    "    M1 = C.view(-1, 1) * C.view(1, -1)   # (2R+1, 2R+1)\n",
    "\n",
    "    C_m1m2 = C_m1m2_sym_def(m, r_vals, N_grid) #C_m1m2_sym(m, r_vals, N_grid) # (M, M, 2R+1)\n",
    "    w = gamma2.view(m.numel(), 1, 1, 1) * gamma2.view(1, m.numel(), 1, 1)   # (M, M, 1, 1)\n",
    "    C_r  = C_m1m2.unsqueeze(-1)   # (M, M, 2R+1, 1)\n",
    "    C_rp = C_m1m2.unsqueeze(-2)   # (M, M, 1, 2R+1)\n",
    "\n",
    "    prod = w * C_r * C_rp\n",
    "    M2 = 2.0 * prod.sum(dim=(0, 1))          # (2R+1, 2R+1)\n",
    "\n",
    "    return M1 + M2\n",
    "\n",
    "R = 5\n",
    "C_avg_analytical = ((std_harm**2).unsqueeze(1) * C_mm(m, torch.arange(-R, R+1), N_grid)).sum(dim=0)\n",
    "M_analytical = second_moment_analytical(R, N_grid)\n",
    "\n",
    "# Visualize the correlation matrix \n",
    "M_analytical[R, R] = 0.0\n",
    "M_analytical = M_analytical.detach().cpu().numpy()\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.imshow(M_analytical, origin='lower', cmap='viridis', aspect='equal')\n",
    "plt.colorbar(label=\"Value\")\n",
    "r_vals = torch.arange(-R, R+1)\n",
    "r_vals = r_vals.cpu().numpy()\n",
    "ticks = np.arange(len(r_vals))\n",
    "plt.xticks(ticks, r_vals)\n",
    "plt.yticks(ticks, r_vals)\n",
    "plt.xlabel(\"r’\")\n",
    "plt.ylabel(\"r\")\n",
    "plt.title(\"Density–density correlation matrix (analytical)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d40282",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dens_dens_corr_func(rho_batch: torch.Tensor, R: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute numerically the correlation function C(r) for each sample in the batch\n",
    "\n",
    "    Args:\n",
    "        rho_batch: tensor of shape (B, N)\n",
    "        R: maximum displacement (integer)\n",
    "\n",
    "    Returns:\n",
    "        C: tensor of shape (B, 2R+1)\n",
    "           C[b, k] = c_b(r) for r = -R + k, k = 0,...,2R\n",
    "    \"\"\"\n",
    "    B, N = rho_batch.shape\n",
    "    Cs = []\n",
    "\n",
    "    for r in range(-R, R + 1):\n",
    "        if r == 0:\n",
    "            prod = rho_batch * rho_batch / N                       # (B, N)\n",
    "        elif r > 0:\n",
    "            # i = 0..N-1-r, i+r = r..N-1\n",
    "            prod = rho_batch[:, :N - r] * rho_batch[:, r:] / (N - r)    # (B, N-r)\n",
    "        else:  # r < 0\n",
    "            k = -r\n",
    "            # i = k..N-1, i+r = i-k = 0..N-1-k\n",
    "            prod = rho_batch[:, k:] * rho_batch[:, :N - k] / (N - k)    # (B, N-k)\n",
    "\n",
    "        C_r = prod.sum(dim=1)                               # (B,)\n",
    "        Cs.append(C_r)\n",
    "        \n",
    "    return torch.stack(Cs, dim=1)\n",
    "\n",
    "def dens_dens_corr_func_sym(rho_batch: torch.Tensor, R: int) -> torch.Tensor:\n",
    "    B, N = rho_batch.shape\n",
    "    Cs = []\n",
    "\n",
    "    for r in range(0, R + 1):\n",
    "        if r == 0:\n",
    "            prod = 0.5 * rho_batch * rho_batch / N                      # (B, N)\n",
    "        else:\n",
    "            prod = 0.5 * (rho_batch[:, :N - r] * rho_batch[:, r:] + rho_batch[:, r:] * rho_batch[:, :N - r]) / (N - r)\n",
    "\n",
    "        C_r = prod.sum(dim=1)                              # (B,)\n",
    "        Cs.append(C_r)\n",
    "        \n",
    "    return torch.stack(Cs, dim=1)\n",
    "\n",
    "def make_symmetric(K_vec: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    K_vec: tensor of shape (R+1,) or (R+1, 1) with entries K(0..R)\n",
    "    Returns: tensor of shape (2R+1,) with K(-R..R), assuming K(-r) = K(r).\n",
    "    \"\"\"\n",
    "    # Make sure it's a 1D tensor\n",
    "    if K_vec.ndim > 1:\n",
    "        K_vec = K_vec.squeeze()\n",
    "\n",
    "    R = K_vec.shape[0] - 1\n",
    "\n",
    "    # K(1..R) reversed → negative side\n",
    "    K_left  = torch.flip(K_vec[1:], dims=[0])\n",
    "    # K(0..R) → non-negative side\n",
    "    K_right = K_vec\n",
    "\n",
    "    K_full = torch.cat([K_left, K_right], dim=0)  # (2R+1,)\n",
    "    return K_full\n",
    "\n",
    "N_cycles = 1\n",
    "N_batch = 1000\n",
    "C_mat = torch.zeros((N_batch, 2*R+1), dtype=dtype, device=device)\n",
    "M = torch.zeros((2*R+1, 2*R+1), dtype=dtype, device=device)\n",
    "for _ in range(N_cycles):\n",
    "\n",
    "    rho_batch, d_rho_batch, a_batch = sample_density_batch(N_batch)  # (B, N)\n",
    "    Eng_batch = E_tot(rho_batch)  # (B,)\n",
    "\n",
    "    C = dens_dens_corr_func(rho_batch, R)  # (B, 2R+1)\n",
    "    C_mat += C  # (B, 2R+1)\n",
    "    M += C.T @ C  # (2R+1, 2R+1)\n",
    "\n",
    "C_avg = C_mat.sum(dim=0) / (N_batch * N_cycles)\n",
    "M = M / (N_batch * N_cycles)\n",
    "\n",
    "r_vals = torch.arange(-R, R+1)\n",
    "plt.plot(r_vals.cpu(), C_mat[0,:].cpu(), 'o-')\n",
    "plt.plot(r_vals.cpu(), C_mat[1,:].cpu(), 'o-')\n",
    "plt.plot(r_vals.cpu(), C_mat[2,:].cpu(), 'o-')\n",
    "plt.plot(r_vals.cpu(), C_mat[3,:].cpu(), 'o-')\n",
    "plt.plot(r_vals.cpu(), C_avg_analytical.cpu(), 'k--', label=\"Average (analytical)\", alpha=0.5)\n",
    "plt.xlabel(\"r\")\n",
    "plt.ylabel(\"C_r = <rho_i rho_{i+r}>\")\n",
    "if data_regime == \"smooth\":\n",
    "    plt.title(\"Correlation function of sampled smooth densities\")\n",
    "else:\n",
    "    plt.title(\"Correlation function of sampled rough densities\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Analyze the eigenspectrum\n",
    "C_symm_mat = dens_dens_corr_func_sym(rho_batch, R)  # (B, R+1)\n",
    "G = C_symm_mat.T @ C_symm_mat / N_batch  # (R+1, R+1)\n",
    "U, S, Vt = torch.svd(G) \n",
    "print(S)\n",
    "\n",
    "# Compute the kernel vector (linear regression) and compare to the true kernel\n",
    "g = (C_symm_mat.T @ Eng_batch) / N_batch\n",
    "k_star = torch.linalg.solve(G, g)\n",
    "print(k_star)\n",
    "k_learned = make_symmetric(k_star)\n",
    "R_max = 10\n",
    "r_grid = torch.arange(-R_max, R_max+1)\n",
    "if kernel_regime == \"single_Gaussian\":\n",
    "    k_true = amp_Gaussian * K_gaussian(r_grid, sigma=sigma_Gaussian)\n",
    "elif kernel_regime == \"double_Gaussian\":\n",
    "    k_true = (amp_Gaussian_1 * K_gaussian(r_grid, sigma=sigma_Gaussian_1) +\n",
    "              amp_Gaussian_2 * K_gaussian(r_grid, sigma=sigma_Gaussian_2))\n",
    "else:\n",
    "    raise ValueError(\"Here kernel_regime must be 'single_Gaussian' or 'double_Gaussian'\")\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(torch.arange(-R, R+1), k_learned , 'o-', label='Learned kernel', linewidth=2)\n",
    "plt.plot(r_grid, k_true, 's--', label='True kernel', alpha=0.7)\n",
    "plt.axhline(0, color='k', linewidth=0.5)\n",
    "plt.xlabel(\"r = i - j\")\n",
    "plt.ylabel(\"K_r\")\n",
    "plt.title(f\"Learned kernel (R = {R})\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize the correlation matrix \n",
    "M[R, R] = 0.0\n",
    "M = M.detach().cpu().numpy()\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.imshow(M, origin='lower', cmap='viridis', aspect='equal')\n",
    "plt.colorbar(label=\"Value\")\n",
    "r_vals = torch.arange(-R, R+1)\n",
    "r_vals = r_vals.cpu().numpy()\n",
    "ticks = np.arange(len(r_vals))\n",
    "plt.xticks(ticks, r_vals)\n",
    "plt.yticks(ticks, r_vals)\n",
    "plt.xlabel(\"r’\")\n",
    "plt.ylabel(\"r\")\n",
    "plt.title(\"Density–density correlation matrix\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Max relative difference between numerical and analytical C(r):\", np.max(np.abs(C_avg.detach().cpu().numpy() - C_avg_analytical.detach().cpu().numpy()))/np.linalg.norm(C_avg_analytical.detach().cpu().numpy()) )\n",
    "print(\"Max relative difference between numerical and analytical correlation matrix:\", np.max(np.abs(M - M_analytical))/np.linalg.norm(M_analytical) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f741e59f",
   "metadata": {},
   "source": [
    "Learning models: \n",
    "(1) Symmetric kernel K_r in the window regime (we learn K_0, K_1, ..., K_R - R is the kernel range)\n",
    "(2) Gaussian mixture kernel K_r = \\sum_n A_n \\exp(-r^2 / sigma_n^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc323535",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearnableKernelConv1d(nn.Module):\n",
    "    \"\"\"\n",
    "    Learnable 1D convolution kernel K_r with range R\n",
    "    Produces phi = [K * rho] (linear convolution with padding)\n",
    "    \"\"\"\n",
    "    def __init__(self, R=5, even_kernel=True, pad_mode=\"zero\"):\n",
    "        super().__init__()\n",
    "        self.R = R\n",
    "        self.pad_mode = pad_mode\n",
    "        self.even_kernel = even_kernel\n",
    "        \n",
    "        if even_kernel:\n",
    "            # learn half + center: w[0] (center), w[1..R] (positive r)\n",
    "            self.kernel_half = nn.Parameter(torch.randn(R+1) * 0.01)\n",
    "        else:\n",
    "            # fully unconstrained kernel of size 2R+1\n",
    "            self.kernel = nn.Parameter(torch.randn(2*R+1) * 0.01)\n",
    "\n",
    "    def build_kernel(self):\n",
    "        \"\"\"\n",
    "        Returns kernel of shape (1,1,2R+1) as required by conv1d\n",
    "        \"\"\"\n",
    "        if self.even_kernel:\n",
    "            center = self.kernel_half[0:1]          # (1,)\n",
    "            pos = self.kernel_half[1:]             # (R,)\n",
    "            neg = pos.flip(0)               # symmetric\n",
    "            full = torch.cat([neg, center, pos], dim=0)  # (2R+1,)\n",
    "        else:\n",
    "            full = self.kernel\n",
    "        return full.view(1,1,-1)  # (out=1, in=1, kernel_size)\n",
    "\n",
    "    def forward(self, rho):\n",
    "        \"\"\"\n",
    "        rho: (B, N_grid)\n",
    "        Returns: phi: (B, N_grid)\n",
    "        \"\"\"\n",
    "        B, N = rho.shape\n",
    "        kernel = self.build_kernel()\n",
    "        kernel = kernel.to(dtype=rho.dtype, device=rho.device)\n",
    "        R = self.R\n",
    "        \n",
    "        if self.pad_mode == \"zero\":\n",
    "            x = F.pad(rho.unsqueeze(1), (R, R), mode='constant', value=0.0)\n",
    "        elif self.pad_mode == \"reflect\":\n",
    "            x = F.pad(rho.unsqueeze(1), (R, R), mode='reflect')\n",
    "        else:\n",
    "            raise ValueError(\"pad_mode must be zero or reflect\")\n",
    "        \n",
    "        phi = F.conv1d(x, kernel, padding=0).squeeze(1)\n",
    "        return phi\n",
    "\n",
    "class KernelOnlyEnergyNN(nn.Module):\n",
    "    \"\"\"\n",
    "    E_tot = (1 / 2N) * sum_{i,j} rho_i rho_j K_{i-j} = (1 / 2N) * sum_{i} rho_i [K * rho]_i\n",
    "    The kernel K is assumed local and learnable via convolution\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, R=5):\n",
    "        super().__init__()\n",
    "        self.R = R\n",
    "        self.kernel_conv = LearnableKernelConv1d(R, even_kernel=True, pad_mode=\"zero\")\n",
    "\n",
    "    def forward(self, features):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features: (B, N_grid, N_feat) - only the first feature (density) is used\n",
    "        \n",
    "        Returns:\n",
    "            local_energies: (B, N_grid) - energy at each grid point\n",
    "            total_energy: (B,) - sum over grid points\n",
    "        \"\"\"\n",
    "        rho_norm = features[..., 0]          # (B, N_grid), isolate density\n",
    "        rho = rho_norm * std_feat[0,0,0] + mean_feat[0,0,0]  # denormalize density\n",
    "\n",
    "        # apply learnable convolution kernel\n",
    "        phi = self.kernel_conv(rho)  # (B, N_grid) # in physical units\n",
    "\n",
    "        local_energies = 0.5 * rho * phi       # (B, N_grid) # physical units\n",
    "        total_energy = local_energies.sum(dim=1) / N_grid       # (B,) # physical units\n",
    "        \n",
    "        total_energy_norm = (total_energy - E_mean) / E_std  # normalize for training\n",
    "\n",
    "        return total_energy_norm\n",
    "    \n",
    "class GaussianMixtureKernelConv1d(nn.Module):\n",
    "    \"\"\"\n",
    "    Learnable 1D kernel K_r represented as a sum of Gaussians:\n",
    "        K(r) = sum_{n=1}^M A_n * exp(-r^2 / sigma_n^2)\n",
    "\n",
    "    Produces phi = [K * rho] via conv1d.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, R: int, n_components: int, pad_mode: str = \"zero\"):\n",
    "        super().__init__()\n",
    "        self.R = R\n",
    "        self.n_components = n_components\n",
    "        self.pad_mode = pad_mode\n",
    "\n",
    "        # r-grid as a buffer: [-R, ..., R] - still is a cutoff in real space\n",
    "        r_vals = torch.arange(-R, R + 1, dtype=dtype)\n",
    "        self.register_buffer(\"r_vals\", r_vals)  # (2R+1,)\n",
    "\n",
    "        # ---- amplitudes A_n (sorted at init) ----\n",
    "        # sample, then sort descending so A_1 >= A_2 >= ... >= A_M\n",
    "        amps = 0.01 * torch.randn(n_components, dtype=dtype)\n",
    "        amps, _ = torch.sort(amps, descending=True)\n",
    "        self.amplitudes = nn.Parameter(amps)\n",
    "\n",
    "        # Log-sigmas so sigma_n = softplus(log_sigma_n) > 0\n",
    "        init_sigmas = torch.arange(1, n_components+1, dtype=dtype)**2\n",
    "        log_sigmas = torch.log(torch.expm1(init_sigmas))  # inverse softplus, so softplus(raw)=init_sigma\n",
    "        self.log_sigmas = nn.Parameter(log_sigmas)\n",
    "\n",
    "    def build_kernel(self):\n",
    "        \"\"\"\n",
    "        Returns kernel of shape (1, 1, 2R+1) as required by conv1d.\n",
    "        \"\"\"\n",
    "        # (2R+1,) -> (1, L)\n",
    "        r = self.r_vals.view(1, -1)              # (1, 2R+1)\n",
    "        r2 = r * r                               # r^2\n",
    "\n",
    "        sigmas = F.softplus(self.log_sigmas) + 1e-8   # (M,)\n",
    "        sigma2 = sigmas.view(-1, 1) ** 2              # (M,1)\n",
    "\n",
    "        # contributions from each Gaussian: (M, L)\n",
    "        # exp(-r^2 / sigma_n^2)\n",
    "        gauss = torch.exp(-r2 / sigma2)          # (M, 2R+1)\n",
    "\n",
    "        # weighted sum over components\n",
    "        kernel_1d = (self.amplitudes.view(-1, 1) * gauss).sum(dim=0)  # (2R+1,)\n",
    "\n",
    "        # conv1d expects (out_channels, in_channels, kernel_size)\n",
    "        return kernel_1d.view(1, 1, -1)\n",
    "\n",
    "    def forward(self, rho: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        rho: (B, N_grid)\n",
    "        Returns: phi = (K * rho): (B, N_grid)\n",
    "        \"\"\"\n",
    "        B, N = rho.shape\n",
    "        kernel = self.build_kernel()\n",
    "        R = self.R\n",
    "\n",
    "        if self.pad_mode == \"zero\":\n",
    "            x = F.pad(rho.unsqueeze(1), (R, R), mode=\"constant\", value=0.0)\n",
    "        elif self.pad_mode == \"reflect\":\n",
    "            x = F.pad(rho.unsqueeze(1), (R, R), mode=\"reflect\")\n",
    "        else:\n",
    "            raise ValueError(\"pad_mode must be 'zero' or 'reflect'\")\n",
    "\n",
    "        phi = F.conv1d(x, kernel, padding=0).squeeze(1)   # (B, N)\n",
    "        return phi\n",
    "\n",
    "\n",
    "class GaussianMixtureEnergyNN(nn.Module):\n",
    "    \"\"\"\n",
    "    E_tot = (1 / 2N) * sum_{i,j} rho_i rho_j K_{i-j}\n",
    "          = (1 / 2N) * sum_i rho_i [K * rho]_i\n",
    "\n",
    "    K is parameterized as a sum of Gaussians in r.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, R=20, n_components=3, pad_mode=\"zero\"):\n",
    "        super().__init__()\n",
    "        self.R = R\n",
    "        self.kernel_conv = GaussianMixtureKernelConv1d(\n",
    "            R=R,\n",
    "            n_components=n_components,\n",
    "            pad_mode=pad_mode,\n",
    "        )\n",
    "\n",
    "    def forward(self, features):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features: (B, N_grid, N_feat) - only the first feature (density) is used\n",
    "\n",
    "        Returns:\n",
    "            total_energy_norm: (B,) - normalized total energy\n",
    "        \"\"\"\n",
    "        # globals: std_feat, mean_feat, E_mean, E_std, N_grid must exist\n",
    "        rho_norm = features[..., 0]          # (B, N_grid)\n",
    "        rho = rho_norm * std_feat[0, 0, 0] + mean_feat[0, 0, 0]\n",
    "\n",
    "        # apply Gaussian-mixture kernel\n",
    "        phi = self.kernel_conv(rho)          # (B, N_grid)\n",
    "\n",
    "        local_energies = 0.5 * rho * phi     # (B, N_grid)\n",
    "        total_energy = local_energies.sum(dim=1) / N_grid    # (B,)\n",
    "\n",
    "        total_energy_norm = (total_energy - E_mean) / E_std\n",
    "        return total_energy_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020745f5",
   "metadata": {},
   "source": [
    "Intermediate routines for training + actual model exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc54438",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    n_batches = 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            y_pred = model(xb)\n",
    "            loss = criterion(y_pred, yb)\n",
    "            total_loss += loss.item()\n",
    "            n_batches += 1\n",
    "    return total_loss / max(1, n_batches)\n",
    "\n",
    "def load_checkpoint(path, model_class, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Loads a saved model checkpoint\n",
    "    Returns:\n",
    "        model: reconstructed and loaded model\n",
    "        normalization: dict of normalization stats\n",
    "        epoch: best epoch\n",
    "        val_loss: best validation loss\n",
    "    \"\"\"\n",
    "    ckpt = torch.load(path, map_location=device)\n",
    "    config = ckpt[\"config\"]\n",
    "    model = model_class(**config).to(device)\n",
    "    model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "    model.eval()\n",
    "\n",
    "    normalization = ckpt.get(\"normalization\", None)\n",
    "    epoch = ckpt.get(\"epoch\", None)\n",
    "    val_loss = ckpt.get(\"val_loss\", None)\n",
    "    return model, normalization, epoch, val_loss\n",
    "    \n",
    "def _run_epoch(model, loader, criterion, train: bool):\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "    running = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    with torch.set_grad_enabled(train):\n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "\n",
    "            if train:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            total_pred = model(xb)               \n",
    "            loss = criterion(total_pred, yb)        \n",
    "\n",
    "            if train:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            running += loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "    return running / max(1, n_batches)\n",
    "\n",
    "def train_with_early_stopping(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    criterion,\n",
    "    scheduler=None,\n",
    "    max_epochs=10000,\n",
    "    patience=10,\n",
    "    min_delta=1e-5,\n",
    "    ckpt_dir=\"checkpoints\",\n",
    "    run_name=None,\n",
    "    learning_regime=\"window\",\n",
    "):\n",
    "    os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "    best_val = math.inf\n",
    "    best_state = None\n",
    "    best_epoch = -1\n",
    "    since_improved = 0\n",
    "\n",
    "    hist = {\"train_loss\": [], \"val_loss\": []}\n",
    "\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        train_loss = _run_epoch(model, train_loader, criterion, train=True)\n",
    "        val_loss = _run_epoch(model, val_loader, criterion, train=False)\n",
    "\n",
    "        hist[\"train_loss\"].append(train_loss)\n",
    "        hist[\"val_loss\"].append(val_loss)\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        improved = (best_val - val_loss) > min_delta\n",
    "        if improved:\n",
    "            best_val = val_loss\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "            best_epoch = epoch\n",
    "            since_improved = 0\n",
    "\n",
    "            # save checkpoint with normalization stats\n",
    "            ckpt_path = os.path.join(ckpt_dir, f\"{run_name}_best.pt\")\n",
    "            if learning_regime == \"window\":\n",
    "                torch.save({\n",
    "                    \"model_state_dict\": best_state,\n",
    "                    \"epoch\": best_epoch,\n",
    "                    \"val_loss\": best_val,\n",
    "                    \"config\": {\n",
    "                        \"R\": model.R,\n",
    "                    },\n",
    "                    \"normalization\": {\n",
    "                        \"mean_feat\": mean_feat.cpu(),\n",
    "                        \"std_feat\":  std_feat.cpu(),\n",
    "                        \"E_mean\":    E_mean.cpu(),\n",
    "                        \"E_std\":     E_std.cpu(),\n",
    "                        \"N_grid\":    int(N_grid),\n",
    "                    }\n",
    "                }, ckpt_path)\n",
    "            elif learning_regime == \"gaussmixt\":\n",
    "                torch.save({\n",
    "                    \"model_state_dict\": best_state,\n",
    "                    \"epoch\": best_epoch,\n",
    "                    \"val_loss\": best_val,\n",
    "                    \"config\": {\n",
    "                        \"n_components\": model.kernel_conv.n_components,                    },\n",
    "                    \"normalization\": {\n",
    "                        \"mean_feat\": mean_feat.cpu(),\n",
    "                        \"std_feat\":  std_feat.cpu(),\n",
    "                        \"E_mean\":    E_mean.cpu(),\n",
    "                        \"E_std\":     E_std.cpu(),\n",
    "                        \"N_grid\":    int(N_grid),\n",
    "                    }\n",
    "                }, ckpt_path)\n",
    "        else:\n",
    "            since_improved += 1\n",
    "\n",
    "        if (epoch % 10) == 0 or epoch == 1:\n",
    "            print(f\"[{epoch:04d}] train={train_loss:.6f} | val={val_loss:.6f} \"\n",
    "                  f\"| best_val={best_val:.6f} (epoch {best_epoch})\")\n",
    "\n",
    "        if since_improved >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch} (best @ {best_epoch}).\")\n",
    "            break\n",
    "\n",
    "    # restore best\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    csv_path = os.path.join(ckpt_dir, f\"{run_name}_history.csv\")\n",
    "    try:\n",
    "        import csv\n",
    "        with open(csv_path, \"w\", newline=\"\") as f:\n",
    "            w = csv.writer(f)\n",
    "            w.writerow([\"epoch\", \"train_loss\", \"val_loss\"])\n",
    "            for i, (tr, va) in enumerate(zip(hist[\"train_loss\"], hist[\"val_loss\"]), start=1):\n",
    "                w.writerow([i, tr, va])\n",
    "    except Exception as e:\n",
    "        print(f\"[warn] could not write CSV: {e}\")\n",
    "\n",
    "    return hist, best_epoch\n",
    "\n",
    "\n",
    "learning_regime = \"gaussmixt\" # \"window\" or \"gaussmixt\"\n",
    "ckpt_dir = \"LearningLocalKernel_checkpoints\"\n",
    "\n",
    "if learning_regime == \"window\":\n",
    "    R_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "    for R in R_list:\n",
    "\n",
    "        run_name = f\"loc_window_kernel_{R}_\" + data_regime + '_' + kernel_regime\n",
    "        torch.manual_seed(1234) # for reproducibility\n",
    "\n",
    "        model = KernelOnlyEnergyNN(R=R).to(device)\n",
    "        \n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        # Reduce LR when val loss plateaus\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=0.5, patience=patience, cooldown=2, min_lr=1e-6\n",
    "        )\n",
    "\n",
    "        hist, best_epoch = train_with_early_stopping(\n",
    "            model,\n",
    "            train_loader,\n",
    "            val_loader,\n",
    "            criterion,\n",
    "            scheduler=scheduler,\n",
    "            max_epochs=N_epochs,\n",
    "            patience=patience,\n",
    "            min_delta=min_delta,\n",
    "            ckpt_dir=ckpt_dir,\n",
    "            run_name=run_name,\n",
    "            learning_regime=learning_regime,\n",
    "        )\n",
    "elif learning_regime == \"gaussmixt\":\n",
    "    n_components_list = [1, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "    for n_components in n_components_list:\n",
    "            \n",
    "        run_name = f\"loc_gaussmix_kernel_{n_components}_\" + data_regime + '_' + kernel_regime\n",
    "        torch.manual_seed(1234) # for reproducibility\n",
    "\n",
    "        model = GaussianMixtureEnergyNN(n_components=n_components).to(device)\n",
    "        \n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        # Reduce LR when val loss plateaus\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=0.5, patience=patience, cooldown=2, min_lr=1e-6\n",
    "        )\n",
    "\n",
    "        hist, best_epoch = train_with_early_stopping(\n",
    "            model,\n",
    "            train_loader,\n",
    "            val_loader,\n",
    "            criterion,\n",
    "            scheduler=scheduler,\n",
    "            max_epochs=N_epochs,\n",
    "            patience=patience,\n",
    "            min_delta=min_delta,\n",
    "            ckpt_dir=ckpt_dir,\n",
    "            run_name=run_name,\n",
    "            learning_regime=learning_regime,\n",
    "        )\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a33478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R = 9\n",
    "# run_name = f\"loc_window_kernel_{R}_\" + data_regime + '_' + kernel_regime\n",
    "\n",
    "n_components = 1\n",
    "run_name = f\"loc_gaussmix_kernel_{n_components}_\" + data_regime + '_' + kernel_regime\n",
    "model, normalization, epoch, val_loss = load_checkpoint(ckpt_dir + f\"/{run_name}_best.pt\", GaussianMixtureEnergyNN, device=device)\n",
    "with torch.no_grad():\n",
    "    amps   = model.kernel_conv.amplitudes.detach().cpu()\n",
    "    sigmas = F.softplus(model.kernel_conv.log_sigmas).detach().cpu()\n",
    "    print(\"amplitudes:\", amps)\n",
    "    print(\"sigmas    :\", sigmas)\n",
    "\n",
    "\n",
    "path = ckpt_dir + f\"/{run_name}_history.csv\"\n",
    "hist_df = pd.read_csv(path)\n",
    "print(hist_df.head())\n",
    "hist_df.plot(x=\"epoch\", y=[\"train_loss\", \"val_loss\"], logy=True, grid=True, title=run_name)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ca2108",
   "metadata": {},
   "outputs": [],
   "source": [
    "R_best = 0\n",
    "best_val = math.inf\n",
    "\n",
    "val_hist_R = []\n",
    "learning_hist_R = []\n",
    "\n",
    "for R in R_list:\n",
    "    run_name = f\"loc_window_kernel_{R}_\" + data_regime + '_' + kernel_regime\n",
    "\n",
    "    model, normalization, epoch, val_loss = load_checkpoint(ckpt_dir + f\"/{run_name}_best.pt\", KernelOnlyEnergyNN, device=device)\n",
    "\n",
    "    val_hist_R.append((R, val_loss))\n",
    "    learning_hist_R.append((R, ))\n",
    "\n",
    "    print(f\"Model {run_name}: best val loss = {val_loss:.6f} at epoch {epoch}\")\n",
    "    if val_loss < best_val:\n",
    "        best_val = val_loss\n",
    "        R_best = R\n",
    "\n",
    "    print(f\"Best model: R={R_best}, val_loss={best_val:.6f}\")\n",
    "\n",
    "print(\"Validation history:\")\n",
    "for R, val_loss in val_hist_R:\n",
    "    print(f\"R={R}: val_loss={val_loss:.6f}\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot([x[0] for x in val_hist_R], [x[1] for x in val_hist_R], 'o-', label='Best validation error', linewidth=2)\n",
    "plt.axhline(0, color='k', linewidth=0.5)\n",
    "plt.xlabel(\"R (kernel range)\")\n",
    "plt.ylabel(\"learning error\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f80f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components_best = 0\n",
    "best_val = math.inf\n",
    "\n",
    "val_hist_n = []\n",
    "learning_hist_n = []\n",
    "\n",
    "for n in n_components_list:\n",
    "    run_name = f\"loc_gaussmix_kernel_{n}_\" + data_regime + '_' + kernel_regime\n",
    "    model, normalization, epoch, val_loss = load_checkpoint(ckpt_dir + f\"/{run_name}_best.pt\", GaussianMixtureEnergyNN, device=device)\n",
    "\n",
    "    val_hist_n.append((n, val_loss))\n",
    "    learning_hist_n.append((n, ))\n",
    "\n",
    "    print(f\"Model {run_name}: best val loss = {val_loss:.6f} at epoch {epoch}\")\n",
    "    if val_loss < best_val:\n",
    "        best_val = val_loss\n",
    "        n_components_best = n\n",
    "\n",
    "    print(f\"Best model: n_components={n_components_best}, val_loss={best_val:.6f}\")\n",
    "\n",
    "print(\"Validation history:\")\n",
    "for n, val_loss in val_hist_n:\n",
    "    print(f\"n_components={n}: val_loss={val_loss:.6f}\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot([x[0] for x in val_hist_n], [x[1] for x in val_hist_n], 'o-', label='Best validation error', linewidth=2)\n",
    "plt.axhline(0, color='k', linewidth=0.5)\n",
    "plt.xlabel(\"n_components (number of Gaussians in kernel)\")\n",
    "plt.ylabel(\"learning error\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d695d3a",
   "metadata": {},
   "source": [
    "Evaluate performance of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3fc477",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "R = 7\n",
    "run_name = f\"loc_window_kernel_{R}_\" + data_regime + '_' + kernel_regime\n",
    "model, normalization, epoch, val_loss = load_checkpoint(ckpt_dir + f\"/{run_name}_best.pt\", KernelOnlyEnergyNN, device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    k_full = model.kernel_conv.build_kernel().view(-1).cpu().numpy() \n",
    "\n",
    "r_grid = np.arange(-R, R+1)\n",
    "\n",
    "if kernel_regime == \"single_Gaussian\":\n",
    "    def K_true(r):\n",
    "        return amp_Gaussian * np.exp(-(r**2) / (sigma_Gaussian**2))\n",
    "elif kernel_regime == \"double_Gaussian\":\n",
    "    def K_true(r):\n",
    "        return (amp_Gaussian_1 * np.exp(-(r**2) / (sigma_Gaussian_1**2)) +\n",
    "                amp_Gaussian_2 * np.exp(-(r**2) / (sigma_Gaussian_2**2)))\n",
    "k_true = K_true(r_grid)\n",
    "\n",
    "n_components = 3\n",
    "run_name = f\"loc_gaussmix_kernel_{n_components}_\" + data_regime + '_' + kernel_regime\n",
    "model, normalization, epoch, val_loss = load_checkpoint(ckpt_dir + f\"/{run_name}_best.pt\", GaussianMixtureEnergyNN, device=device)  \n",
    "with torch.no_grad():\n",
    "    r_grid_gm = model.kernel_conv.r_vals.cpu().numpy()       # shape (2R+1,)\n",
    "    k_full_gm = model.kernel_conv.build_kernel().view(-1).cpu().numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(r_grid, k_full , 'o-', label='Learned kernel (window)', linewidth=2)\n",
    "plt.plot(r_grid, k_true, 's--', label='True kernel', alpha=0.7)\n",
    "plt.plot(r_grid_gm, k_full_gm , 'x-', label='Learned kernel (gaussmix)', linewidth=2)\n",
    "plt.axhline(0, color='k', linewidth=0.5)\n",
    "plt.xlabel(\"r = i - j\")\n",
    "plt.ylabel(\"K_r\")\n",
    "plt.title(f\"Learned kernel (R = {R})\")\n",
    "plt.xlim(-R, R)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebf9ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_train_norm\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(targets_train_norm, bins=50)\n",
    "plt.xlabel(\"targets_train_norm\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.title(\"Histogram of targets_train_norm\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f504926",
   "metadata": {},
   "outputs": [],
   "source": [
    "def E_int_conv_discrete(rho: torch.Tensor,\n",
    "                        k_full: torch.Tensor,\n",
    "                        R: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Interaction energy with a finite-range kernel k_full of length 2R+1.\n",
    "    rho: (N,) or (B, N)\n",
    "    k_full: (2R+1,)  -- symmetric kernel [K_-R, ..., K_0, ..., K_R]\n",
    "    R: kernel range\n",
    "    Returns: (B,) energy\n",
    "    \"\"\"\n",
    "    if rho.dim() == 1:\n",
    "        rho = rho.unsqueeze(0)  # (1, N)\n",
    "    B, N = rho.shape\n",
    "    device, dtype = rho.device, rho.dtype\n",
    "\n",
    "    k_full = k_full.to(device=device, dtype=dtype)          # (2R+1,)\n",
    "    weight = k_full.view(1, 1, -1)                          # (1, 1, 2R+1)\n",
    "\n",
    "    # pad by R on each side for linear convolution with kernel of size 2R+1\n",
    "    u = F.conv1d(rho.unsqueeze(1), weight, padding=R).squeeze(1)  # (B, N)\n",
    "\n",
    "    E = 0.5 * (rho * u).sum(dim=-1) / N  # (B,)\n",
    "    return E.squeeze(0) if E.numel() == 1 else E\n",
    "\n",
    "\n",
    "rho_batch, d_rho_batch_analytical, _ = sample_density_batch(32)  # (B, N_grid)\n",
    "\n",
    "if kernel_regime == \"single_Gaussian\":\n",
    "    E_int_batch = amp_Gaussian * E_int_conv(rho_batch, kernel=\"gaussian\", sigma=sigma_Gaussian)\n",
    "    E_int_batch_discrete = E_int_conv_discrete(rho_batch, model.kernel_conv.build_kernel().view(-1), R=model.R)\n",
    "elif kernel_regime == \"double_Gaussian\":\n",
    "    E_int_batch = (amp_Gaussian_1 * E_int_conv(rho_batch, kernel=\"gaussian\", sigma=sigma_Gaussian_1) +\n",
    "                   amp_Gaussian_2 * E_int_conv(rho_batch, kernel=\"gaussian\", sigma=sigma_Gaussian_2))\n",
    "    E_int_batch_discrete = E_int_conv_discrete(rho_batch, model.kernel_conv.build_kernel().view(-1), R=model.R)\n",
    "E_int_batch_discrete = E_int_conv_discrete(rho_batch, model.kernel_conv.build_kernel().view(-1), R=model.R)\n",
    "print(\"E_int_batch:\", E_int_batch)\n",
    "print(\"E_int_batch_discrete:\", E_int_batch_discrete)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1049be71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
