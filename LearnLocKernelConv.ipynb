{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75ad941b",
   "metadata": {},
   "source": [
    "Total energy now contains local density-density interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8622f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import math, os, time, copy\n",
    "import torch.fft as tfft\n",
    "import pandas as pd\n",
    "import torch_dct as dct\n",
    "from numpy import size\n",
    "from ewaldnn1d import *\n",
    "\n",
    "torch.random.manual_seed(1234) # for reproducibility\n",
    "\n",
    "# Global settings\n",
    "dtype = torch.float64\n",
    "device = \"cpu\"\n",
    "\n",
    "data_regime = \"smooth\" # \"smooth\" or \"rough\"\n",
    "\n",
    "N_grid = 512 # number of grid points\n",
    "if data_regime == \"smooth\":\n",
    "    M_cutoff = 50 # maximum harmonic\n",
    "    m = torch.arange(1, M_cutoff+1, dtype=dtype, device=device)             # (M,)\n",
    "    x = torch.linspace(0, 1, N_grid, dtype=dtype, device=device)            # (N,)\n",
    "    #design matrix needed to sample densities\n",
    "    DesignMatrix = torch.cos(torch.pi * torch.outer(m, x))                  # (M, N)\n",
    "    DerDM = -torch.pi * m[:, None] * torch.sin(torch.pi * torch.outer(m, x))  # (M, N) # derivative of design matrix\n",
    "    std_harm = 2.0 / (1.0 + 0.2 * m)**2 # std_harm = 2.0 / (1.0 + m)**2\n",
    "elif data_regime == \"rough\":\n",
    "    M_cutoff = N_grid - 1 # maximum harmonic\n",
    "    m = torch.arange(1, M_cutoff+1, dtype=dtype, device=device)             # (M,)\n",
    "    x = torch.linspace(0, 1, N_grid, dtype=dtype, device=device)            # (N,)\n",
    "    #design matrix needed to sample densities\n",
    "    DesignMatrix = torch.cos(torch.pi * torch.outer(m, x))                  # (M, N)\n",
    "    DerDM = -torch.pi * m[:, None] * torch.sin(torch.pi * torch.outer(m, x))  # (M, N) # derivative of design matrix\n",
    "    std_harm = 2.0 / (1.0 + 0.0 * m)**2\n",
    "else:\n",
    "    raise ValueError(\"regime must be 'smooth' or 'rough'\")\n",
    "\n",
    "N_train = 1500\n",
    "N_test = 250\n",
    "N_val = 250\n",
    "\n",
    "N_batch = 50\n",
    "N_epochs = 10000\n",
    "lr = 1e-2 # we will use a LR scheduler, so this is just an initial value\n",
    "min_delta = 1e-5 # min change in the monitored quantity to qualify as an improvement\n",
    "patience = 30    # epochs to wait for improvement before stopping training'\n",
    "pad_mode = \"zero\" # padding mode for convolution-based routines: either \"zero\" or \"reflect\"\n",
    "N_feat = 1 # number of local features per grid point\n",
    "\n",
    "# interaction kernel parameters\n",
    "kernel_regime = \"double_Gaussian\" # \"single_Gaussian\" or \"double_Gaussian\" \n",
    "if kernel_regime == \"single_Gaussian\":\n",
    "    amp_Gaussian = 1.0 # amplitude of Gaussian kernel for interaction energy\n",
    "    sigma_Gaussian = 3.0 # width of Gaussian kernel for interaction energy\n",
    "elif kernel_regime == \"double_Gaussian\":\n",
    "    amp_Gaussian_1 = -1.0 # amplitude of first Gaussian kernel for interaction energy\n",
    "    sigma_Gaussian_1 = 3.0 # width of first Gaussian kernel for interaction energy\n",
    "    amp_Gaussian_2 = 2.0 # amplitude of second Gaussian kernel for interaction energy\n",
    "    sigma_Gaussian_2 = 1.0 # width of second Gaussian kernel for interaction energy\n",
    "else:\n",
    "    raise ValueError(\"Here kernel_regime must be 'single_Gaussian' or 'double_Gaussian'\")\n",
    "\n",
    "\n",
    "if kernel_regime == \"single_Gaussian\":\n",
    "    def E_tot(rho: torch.Tensor) -> torch.Tensor:\n",
    "        return amp_Gaussian * E_int_conv(rho, kernel=\"gaussian\", sigma=sigma_Gaussian, pad_mode=pad_mode)\n",
    "elif kernel_regime == \"double_Gaussian\":\n",
    "    def E_tot(rho: torch.Tensor) -> torch.Tensor:\n",
    "        return (amp_Gaussian_1 * E_int_conv(rho, kernel=\"gaussian\", sigma=sigma_Gaussian_1, pad_mode=pad_mode) +\n",
    "                amp_Gaussian_2 * E_int_conv(rho, kernel=\"gaussian\", sigma=sigma_Gaussian_2, pad_mode=pad_mode))\n",
    "    \n",
    "# generate train/test split\n",
    "rho_train, d_rho_train, a_train = sample_density_batch(N_train, std_harm=std_harm, DesignMatrix=DesignMatrix, DerDM=DerDM)  # (N_train, N_grid)\n",
    "rho_test, d_rho_test, a_test = sample_density_batch(N_test, std_harm=std_harm, DesignMatrix=DesignMatrix, DerDM=DerDM)   # (N_test, N_grid)\n",
    "rho_val, d_rho_val, a_val = sample_density_batch(N_val, std_harm=std_harm, DesignMatrix=DesignMatrix, DerDM=DerDM)    # (N_val, N_grid)\n",
    "features_train = generate_loc_features_rs(rho_train, N_feat=N_feat)  # (N_train, N_grid, N_feat)\n",
    "features_test  = generate_loc_features_rs(rho_test, N_feat=N_feat)   # (N_test, N_grid, N_feat)\n",
    "features_val   = generate_loc_features_rs(rho_val, N_feat=N_feat)    # (N_val, N_grid, N_feat)\n",
    "\n",
    "targets_train = E_tot(rho_train)            # (N_train,)\n",
    "targets_test  = E_tot(rho_test)             # (N_test,)\n",
    "targets_val   = E_tot(rho_val)              # (N_val,)\n",
    "\n",
    "# Normalize features\n",
    "mean_feat, std_feat = compute_normalization_stats(features_train)\n",
    "features_train_norm = normalize_features(features_train, mean_feat, std_feat)\n",
    "features_test_norm = normalize_features(features_test, mean_feat, std_feat)\n",
    "features_val_norm = normalize_features(features_val, mean_feat, std_feat)\n",
    "\n",
    "# Normalize targets\n",
    "E_mean = targets_train.mean()\n",
    "E_std = targets_train.std()\n",
    "targets_train_norm = (targets_train - E_mean) / E_std\n",
    "targets_test_norm = (targets_test - E_mean) / E_std\n",
    "targets_val_norm = (targets_val - E_mean) / E_std\n",
    "\n",
    "# Datasets\n",
    "train_dataset = TensorDataset(features_train_norm, targets_train_norm)\n",
    "val_dataset   = TensorDataset(features_val_norm,   targets_val_norm)\n",
    "test_dataset  = TensorDataset(features_test_norm,  targets_test_norm)\n",
    "\n",
    "# Loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=N_batch, shuffle=True,  drop_last=False)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=N_batch, shuffle=False, drop_last=False)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=N_batch, shuffle=False, drop_last=False)\n",
    "\n",
    "\n",
    "R = 10 \n",
    "r_grid = torch.arange(-R, R+1)\n",
    "plt.figure(figsize=(6,4))\n",
    "if kernel_regime == \"single_Gaussian\":\n",
    "    plt.plot(r_grid, amp_Gaussian * K_gaussian(r_grid, sigma=sigma_Gaussian), 'o-', linewidth=2)\n",
    "elif kernel_regime == \"double_Gaussian\":\n",
    "    plt.plot(r_grid, amp_Gaussian_1 * K_gaussian(r_grid, sigma=sigma_Gaussian_1) + amp_Gaussian_2 * K_gaussian(r_grid, sigma=sigma_Gaussian_2), 'o-', linewidth=2)\n",
    "plt.axhline(0, color='k', linewidth=0.5)\n",
    "plt.xlabel(\"r = i - j\")\n",
    "plt.ylabel(\"K_r\")\n",
    "plt.title(f\"Local kernel\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "N_batch = 20\n",
    "rho_batch, _, a_batch = sample_density_batch(N_batch, std_harm=std_harm, DesignMatrix=DesignMatrix, DerDM=DerDM)\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(x.numpy(), rho_batch[0, :].numpy(), lw=2)\n",
    "plt.plot(x.numpy(), rho_batch[1, :].numpy(), lw=2)\n",
    "plt.plot(x.numpy(), rho_batch[2, :].numpy(), lw=2)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"rho(x)\")\n",
    "plt.title(\"Random sampled densities from cosine basis\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# For saving data used in Figure 1\n",
    "# np.savetxt(\"Figure_1/rho_batch_\" + data_regime + \".txt\", rho_batch.cpu().numpy())\n",
    "# R = 25\n",
    "# C_avg_analytical = ((std_harm**2).unsqueeze(1) * C_mm(m, torch.arange(-R, R+1), N_grid)).sum(dim=0)\n",
    "# r_vals = torch.arange(-R, R+1)\n",
    "# data = np.column_stack([r_vals.cpu().numpy(), C_avg_analytical.cpu().numpy()])\n",
    "# np.savetxt(\"Figure_1/C_avg_\" + data_regime + \".txt\", data)\n",
    "# C = dens_dens_corr_func(rho_batch, R)\n",
    "# np.savetxt(\"Figure_1/C_batch_\" + data_regime + \".txt\", C.cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9eed5b",
   "metadata": {},
   "source": [
    "Density-density correlation function analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d40013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analytical calculation of the correlation matrix\n",
    "R = 5\n",
    "C_avg_analytical = ((std_harm**2).unsqueeze(1) * C_mm(m, torch.arange(-R, R+1), N_grid)).sum(dim=0)\n",
    "M_analytical = second_moment_analytical(m, R, N_grid, std_harm)\n",
    "flag_zero = True\n",
    "\n",
    "# Visualize the correlation matrix \n",
    "if data_regime == \"rough\" and flag_zero:\n",
    "    M_analytical[R, R] = 0.0\n",
    "M_analytical = M_analytical.detach().cpu().numpy()\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.imshow(M_analytical, origin='lower', cmap='viridis', aspect='equal')\n",
    "plt.colorbar(label=\"Value\")\n",
    "r_vals = torch.arange(-R, R+1)\n",
    "r_vals = r_vals.cpu().numpy()\n",
    "ticks = np.arange(len(r_vals))\n",
    "plt.xticks(ticks, r_vals)\n",
    "plt.yticks(ticks, r_vals)\n",
    "plt.xlabel(\"r’\")\n",
    "plt.ylabel(\"r\")\n",
    "plt.title(\"Density–density correlation matrix (analytical)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d40282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical estimation of the correlation matrix\n",
    "N_cycles = 1\n",
    "N_batch = 1000\n",
    "C_mat = torch.zeros((N_batch, 2*R+1), dtype=dtype, device=device)\n",
    "M = torch.zeros((2*R+1, 2*R+1), dtype=dtype, device=device)\n",
    "for _ in range(N_cycles):\n",
    "\n",
    "    rho_batch, d_rho_batch, a_batch = sample_density_batch(N_batch, std_harm=std_harm, DesignMatrix=DesignMatrix, DerDM=DerDM)  # (B, N)\n",
    "    Eng_batch = E_tot(rho_batch)  # (B,)\n",
    "\n",
    "    C = dens_dens_corr_func(rho_batch, R)  # (B, 2R+1)\n",
    "    C_mat += C  # (B, 2R+1)\n",
    "    M += C.T @ C  # (2R+1, 2R+1)\n",
    "\n",
    "C_avg = C_mat.sum(dim=0) / (N_batch * N_cycles)\n",
    "M = M / (N_batch * N_cycles)\n",
    "\n",
    "r_vals = torch.arange(-R, R+1)\n",
    "plt.plot(r_vals.cpu(), C_mat[0,:].cpu(), 'o-')\n",
    "plt.plot(r_vals.cpu(), C_mat[1,:].cpu(), 'o-')\n",
    "plt.plot(r_vals.cpu(), C_mat[2,:].cpu(), 'o-')\n",
    "plt.plot(r_vals.cpu(), C_mat[3,:].cpu(), 'o-')\n",
    "plt.plot(r_vals.cpu(), C_avg_analytical.cpu(), 'k--', label=\"Average (analytical)\", alpha=0.5)\n",
    "plt.xlabel(\"r\")\n",
    "plt.ylabel(\"C_r = <rho_i rho_{i+r}>\")\n",
    "if data_regime == \"smooth\":\n",
    "    plt.title(\"Correlation function of sampled smooth densities\")\n",
    "else:\n",
    "    plt.title(\"Correlation function of sampled rough densities\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Analyze the eigenspectrum\n",
    "C_symm_mat = dens_dens_corr_func_sym(rho_batch, R)  # (B, R+1)\n",
    "G = C_symm_mat.T @ C_symm_mat / N_batch  # (R+1, R+1)\n",
    "U, S, Vt = torch.svd(G) \n",
    "print(\"Eigenspectrum:\", S)\n",
    "\n",
    "# Compute the kernel vector (linear regression) and compare to the true kernel\n",
    "g = (C_symm_mat.T @ Eng_batch) / N_batch\n",
    "k_star = torch.linalg.solve(G, g)\n",
    "print(\"Learned kernel vector:\", k_star)\n",
    "k_learned = make_symmetric(k_star)\n",
    "R_max = 10\n",
    "r_grid = torch.arange(-R_max, R_max+1)\n",
    "if kernel_regime == \"single_Gaussian\":\n",
    "    k_true = amp_Gaussian * K_gaussian(r_grid, sigma=sigma_Gaussian)\n",
    "elif kernel_regime == \"double_Gaussian\":\n",
    "    k_true = (amp_Gaussian_1 * K_gaussian(r_grid, sigma=sigma_Gaussian_1) +\n",
    "              amp_Gaussian_2 * K_gaussian(r_grid, sigma=sigma_Gaussian_2))\n",
    "else:\n",
    "    raise ValueError(\"Here kernel_regime must be 'single_Gaussian' or 'double_Gaussian'\")\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(torch.arange(-R, R+1), k_learned , 'o-', label='Learned kernel', linewidth=2)\n",
    "plt.plot(r_grid, k_true, 's--', label='True kernel', alpha=0.7)\n",
    "plt.axhline(0, color='k', linewidth=0.5)\n",
    "plt.xlabel(\"r = i - j\")\n",
    "plt.ylabel(\"K_r\")\n",
    "plt.title(f\"Learned kernel (R = {R})\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize the correlation matrix \n",
    "if data_regime == \"rough\" and flag_zero:\n",
    "    M[R, R] = 0.0\n",
    "M = M.detach().cpu().numpy()\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.imshow(M, origin='lower', cmap='viridis', aspect='equal')\n",
    "plt.colorbar(label=\"Value\")\n",
    "r_vals = torch.arange(-R, R+1)\n",
    "r_vals = r_vals.cpu().numpy()\n",
    "ticks = np.arange(len(r_vals))\n",
    "plt.xticks(ticks, r_vals)\n",
    "plt.yticks(ticks, r_vals)\n",
    "plt.xlabel(\"r’\")\n",
    "plt.ylabel(\"r\")\n",
    "plt.title(\"Density–density correlation matrix (numerical)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Max relative difference between numerical and analytical C(r):\", np.max(np.abs(C_avg.detach().cpu().numpy() - C_avg_analytical.detach().cpu().numpy()))/np.linalg.norm(C_avg_analytical.detach().cpu().numpy()) )\n",
    "print(\"Max relative difference between numerical and analytical correlation matrix:\", np.max(np.abs(M - M_analytical))/np.linalg.norm(M_analytical) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020745f5",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc54438",
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_train = False # set to True to train models\n",
    "learning_regime = \"window\" # \"window\" or \"gaussmixt\"\n",
    "ckpt_dir = \"LearningLocalKernel_checkpoints\"\n",
    "\n",
    "if flag_train:\n",
    "    if learning_regime == \"window\":\n",
    "        R_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "        for R in R_list:\n",
    "\n",
    "            run_name = f\"loc_window_kernel_{R}_\" + data_regime + '_' + kernel_regime\n",
    "            torch.manual_seed(1234) # for reproducibility\n",
    "\n",
    "            model = KernelOnlyEnergyNN(\n",
    "                R=R, \n",
    "                pad_mode=pad_mode,\n",
    "                mean_feat=mean_feat,\n",
    "                std_feat=std_feat,\n",
    "                E_mean=E_mean,\n",
    "                E_std=E_std,\n",
    "            ).to(device)\n",
    "            \n",
    "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "            criterion = nn.MSELoss()\n",
    "\n",
    "            # Reduce LR when val loss plateaus\n",
    "            scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer, mode='min', factor=0.5, patience=patience, cooldown=2, min_lr=1e-6\n",
    "            )\n",
    "\n",
    "\n",
    "            hist, best_epoch = train_with_early_stopping(\n",
    "                model=model,\n",
    "                train_loader=train_loader,\n",
    "                val_loader=val_loader,\n",
    "                criterion=criterion,\n",
    "                optimizer=optimizer,\n",
    "                scheduler=scheduler,\n",
    "                max_epochs=N_epochs,\n",
    "                patience=patience,\n",
    "                min_delta=min_delta,\n",
    "                ckpt_dir=ckpt_dir,\n",
    "                run_name=run_name,\n",
    "                learning_regime=learning_regime,\n",
    "                N_grid=N_grid,\n",
    "                device=device,\n",
    "            )\n",
    "\n",
    "    elif learning_regime == \"gaussmixt\":\n",
    "        n_components_list = [1, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "        for n_components in n_components_list:\n",
    "                \n",
    "            run_name = f\"loc_gaussmix_kernel_{n_components}_\" + data_regime + '_' + kernel_regime\n",
    "            torch.manual_seed(1234) # for reproducibility\n",
    "\n",
    "            model = GaussianMixtureEnergyNN(\n",
    "                R=40,\n",
    "                n_components=n_components,\n",
    "                pad_mode=pad_mode,\n",
    "                mean_feat=mean_feat,\n",
    "                std_feat=std_feat,\n",
    "                E_mean=E_mean,\n",
    "                E_std=E_std,\n",
    "            ).to(device)\n",
    "\n",
    "            \n",
    "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "            criterion = nn.MSELoss()\n",
    "\n",
    "            # Reduce LR when val loss plateaus\n",
    "            scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer, mode='min', factor=0.5, patience=patience, cooldown=2, min_lr=1e-6\n",
    "            )\n",
    "\n",
    "            hist, best_epoch = train_with_early_stopping(\n",
    "                model=model,\n",
    "                train_loader=train_loader,\n",
    "                val_loader=val_loader,\n",
    "                criterion=criterion,\n",
    "                optimizer=optimizer,\n",
    "                scheduler=scheduler,\n",
    "                max_epochs=N_epochs,\n",
    "                patience=patience,\n",
    "                min_delta=min_delta,\n",
    "                ckpt_dir=ckpt_dir,\n",
    "                run_name=run_name,\n",
    "                learning_regime=learning_regime,\n",
    "                N_grid=N_grid,\n",
    "                device=device,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a33478",
   "metadata": {},
   "outputs": [],
   "source": [
    "if learning_regime == \"window\":\n",
    "    R = 10\n",
    "    run_name = f\"loc_window_kernel_{R}_\" + data_regime + '_' + kernel_regime\n",
    "elif learning_regime == \"gaussmixt\":\n",
    "    n_components = 3\n",
    "    run_name = f\"loc_gaussmix_kernel_{n_components}_\" + data_regime + '_' + kernel_regime\n",
    "    model, normalization, epoch, val_loss = load_checkpoint(ckpt_dir + f\"/{run_name}_best.pt\", GaussianMixtureEnergyNN, device=device)\n",
    "    with torch.no_grad():\n",
    "        amps   = model.kernel_conv.amplitudes.detach().cpu()\n",
    "        sigmas = F.softplus(model.kernel_conv.log_sigmas).detach().cpu()\n",
    "        print(\"amplitudes:\", amps)\n",
    "        print(\"sigmas    :\", sigmas)\n",
    "\n",
    "\n",
    "path = ckpt_dir + f\"/{run_name}_history.csv\"\n",
    "hist_df = pd.read_csv(path)\n",
    "print(hist_df.head())\n",
    "hist_df.plot(x=\"epoch\", y=[\"train_loss\", \"val_loss\"], logy=True, grid=True, title=run_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ca2108",
   "metadata": {},
   "outputs": [],
   "source": [
    "if learning_regime == \"window\":\n",
    "    R_best = 0\n",
    "    best_val = math.inf\n",
    "\n",
    "    val_hist_R = []\n",
    "    learning_hist_R = []\n",
    "\n",
    "    for R in R_list:\n",
    "        run_name = f\"loc_window_kernel_{R}_\" + data_regime + '_' + kernel_regime\n",
    "\n",
    "        model, normalization, epoch, val_loss = load_checkpoint(ckpt_dir + f\"/{run_name}_best.pt\", KernelOnlyEnergyNN, device=device)\n",
    "\n",
    "        val_hist_R.append((R, val_loss))\n",
    "        learning_hist_R.append((R, ))\n",
    "\n",
    "        print(f\"Model {run_name}: best val loss = {val_loss:.6f} at epoch {epoch}\")\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            R_best = R\n",
    "\n",
    "        print(f\"Best model: R={R_best}, val_loss={best_val:.6f}\")\n",
    "\n",
    "    print(\"Validation history:\")\n",
    "    for R, val_loss in val_hist_R:\n",
    "        print(f\"R={R}: val_loss={val_loss:.6f}\")\n",
    "\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot([x[0] for x in val_hist_R], [x[1] for x in val_hist_R], 'o-', label='Best validation error', linewidth=2)\n",
    "    plt.axhline(0, color='k', linewidth=0.5)\n",
    "    plt.xlabel(\"R (kernel range)\")\n",
    "    plt.ylabel(\"learning error\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "elif learning_regime == \"gaussmixt\":\n",
    "    n_components_best = 0\n",
    "    best_val = math.inf\n",
    "\n",
    "    val_hist_n = []\n",
    "    learning_hist_n = []\n",
    "\n",
    "    for n in n_components_list:\n",
    "        run_name = f\"loc_gaussmix_kernel_{n}_\" + data_regime + '_' + kernel_regime\n",
    "        model, normalization, epoch, val_loss = load_checkpoint(ckpt_dir + f\"/{run_name}_best.pt\", GaussianMixtureEnergyNN, device=device)\n",
    "\n",
    "        val_hist_n.append((n, val_loss))\n",
    "        learning_hist_n.append((n, ))\n",
    "\n",
    "        print(f\"Model {run_name}: best val loss = {val_loss:.6f} at epoch {epoch}\")\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            n_components_best = n\n",
    "\n",
    "        print(f\"Best model: n_components={n_components_best}, val_loss={best_val:.6f}\")\n",
    "\n",
    "    print(\"Validation history:\")\n",
    "    for n, val_loss in val_hist_n:\n",
    "        print(f\"n_components={n}: val_loss={val_loss:.6f}\")\n",
    "\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot([x[0] for x in val_hist_n], [x[1] for x in val_hist_n], 'o-', label='Best validation error', linewidth=2)\n",
    "    plt.axhline(0, color='k', linewidth=0.5)\n",
    "    plt.xlabel(\"n_components (number of Gaussians in kernel)\")\n",
    "    plt.ylabel(\"learning error\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d695d3a",
   "metadata": {},
   "source": [
    "Evaluate performance of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3fc477",
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_window = True # set to True to visualize window-based kernel\n",
    "flag_gaussmix = True # set to True to visualize gaussmixt-based kernel\n",
    "\n",
    "R = 10\n",
    "n_components = 3\n",
    "# Original true kernel\n",
    "r_grid = np.arange(-R, R+1)\n",
    "if kernel_regime == \"single_Gaussian\":\n",
    "    def K_true(r):\n",
    "        return amp_Gaussian * np.exp(-(r**2) / (sigma_Gaussian**2))\n",
    "elif kernel_regime == \"double_Gaussian\":\n",
    "    def K_true(r):\n",
    "        return (amp_Gaussian_1 * np.exp(-(r**2) / (sigma_Gaussian_1**2)) +\n",
    "                amp_Gaussian_2 * np.exp(-(r**2) / (sigma_Gaussian_2**2)))\n",
    "k_true = K_true(r_grid)\n",
    "\n",
    "if flag_window:\n",
    "    run_name = f\"loc_window_kernel_{R}_\" + data_regime + '_' + kernel_regime\n",
    "    model, normalization, epoch, val_loss = load_checkpoint(ckpt_dir + f\"/{run_name}_best.pt\", KernelOnlyEnergyNN, device=device)\n",
    "    with torch.no_grad():\n",
    "        k_full = model.kernel_conv.build_kernel().view(-1).cpu().numpy() \n",
    "if flag_gaussmix:\n",
    "    run_name = f\"loc_gaussmix_kernel_{n_components}_\" + data_regime + '_' + kernel_regime\n",
    "    model, normalization, epoch, val_loss = load_checkpoint(ckpt_dir + f\"/{run_name}_best.pt\", GaussianMixtureEnergyNN, device=device)  \n",
    "    with torch.no_grad():\n",
    "        r_grid_gm = model.kernel_conv.r_vals.cpu().numpy()       # shape (2R+1,)\n",
    "        k_full_gm = model.kernel_conv.build_kernel().view(-1).cpu().numpy()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(r_grid, k_true, 's--', label='True kernel', alpha=0.7)\n",
    "if flag_window:\n",
    "    plt.plot(r_grid, k_full , 'o-', label='Learned kernel (window)', linewidth=2)\n",
    "if flag_gaussmix:\n",
    "    plt.plot(r_grid_gm, k_full_gm , 'x-', label='Learned kernel (gaussmix)', linewidth=2)\n",
    "plt.axhline(0, color='k', linewidth=0.5)\n",
    "plt.xlabel(\"r = i - j\")\n",
    "plt.ylabel(\"K_r\")\n",
    "plt.xlim(-R, R)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# For saving data used in Figure 1\n",
    "# data = np.column_stack([r_grid, k_true, k_full])\n",
    "# np.savetxt(\"Figure_1/kernels_window_\" + data_regime + \".txt\", data)\n",
    "# data = np.column_stack([r_grid_gm, k_full_gm])\n",
    "# np.savetxt(\"Figure_1/kernels_gaussmix_\" + data_regime + \".txt\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebf9ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_train_norm\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(targets_train_norm, bins=50)\n",
    "plt.xlabel(\"targets_train_norm\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.title(\"Histogram of targets_train_norm\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f504926",
   "metadata": {},
   "outputs": [],
   "source": [
    "if learning_regime == \"window\":\n",
    "# energy from the learned window kernel\n",
    "    def E_int_conv_discrete(rho: torch.Tensor,\n",
    "                            k_full: torch.Tensor,\n",
    "                            R: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Interaction energy with a finite-range kernel k_full of length 2R+1.\n",
    "        rho: (N,) or (B, N)\n",
    "        k_full: (2R+1,)  -- symmetric kernel [K_-R, ..., K_0, ..., K_R]\n",
    "        R: kernel range\n",
    "        Returns: (B,) energy\n",
    "        \"\"\"\n",
    "        if rho.dim() == 1:\n",
    "            rho = rho.unsqueeze(0)  # (1, N)\n",
    "        B, N = rho.shape\n",
    "        device, dtype = rho.device, rho.dtype\n",
    "\n",
    "        k_full = k_full.to(device=device, dtype=dtype)          # (2R+1,)\n",
    "        weight = k_full.view(1, 1, -1)                          # (1, 1, 2R+1)\n",
    "\n",
    "        # pad by R on each side for linear convolution with kernel of size 2R+1\n",
    "        u = F.conv1d(rho.unsqueeze(1), weight, padding=R).squeeze(1)  # (B, N)\n",
    "\n",
    "        E = 0.5 * (rho * u).sum(dim=-1) / N  # (B,)\n",
    "        return E.squeeze(0) if E.numel() == 1 else E\n",
    "\n",
    "    N_batch = 5\n",
    "    rho_batch, _, _ = sample_density_batch(N_batch, std_harm=std_harm, DesignMatrix=DesignMatrix, DerDM=DerDM)  # (B, N_grid)\n",
    "\n",
    "    if kernel_regime == \"single_Gaussian\":\n",
    "        E_int_batch = amp_Gaussian * E_int_conv(rho_batch, kernel=\"gaussian\", sigma=sigma_Gaussian)\n",
    "        E_int_batch_discrete = E_int_conv_discrete(rho_batch, model.kernel_conv.build_kernel().view(-1), R=model.R)\n",
    "    elif kernel_regime == \"double_Gaussian\":\n",
    "        E_int_batch = (amp_Gaussian_1 * E_int_conv(rho_batch, kernel=\"gaussian\", sigma=sigma_Gaussian_1) +\n",
    "                    amp_Gaussian_2 * E_int_conv(rho_batch, kernel=\"gaussian\", sigma=sigma_Gaussian_2))\n",
    "        E_int_batch_discrete = E_int_conv_discrete(rho_batch, model.kernel_conv.build_kernel().view(-1), R=model.R)\n",
    "    E_int_batch_discrete = E_int_conv_discrete(rho_batch, model.kernel_conv.build_kernel().view(-1), R=model.R)\n",
    "    print(\"E_int_batch:\", E_int_batch)\n",
    "    print(\"E_int_batch_discrete:\", E_int_batch_discrete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b6d079",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
