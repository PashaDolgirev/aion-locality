{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8622f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import math, os, time, copy\n",
    "import torch.fft as tfft\n",
    "import pandas as pd\n",
    "import torch_dct as dct\n",
    "from numpy import size\n",
    "from ewaldnn1d import *\n",
    "\n",
    "torch.random.manual_seed(1234) # for reproducibility\n",
    "\n",
    "# Global settings\n",
    "dtype = torch.float64\n",
    "device = \"cpu\"\n",
    "\n",
    "data_regime = \"rough\" # \"smooth\" or \"rough\"\n",
    "\n",
    "N_grid = 512 # number of grid points\n",
    "if data_regime == \"smooth\":\n",
    "    M_cutoff = 50 # maximum harmonic\n",
    "    m = torch.arange(1, M_cutoff+1, dtype=dtype, device=device)             # (M,)\n",
    "    x = torch.linspace(0, 1, N_grid, dtype=dtype, device=device)            # (N,)\n",
    "    #design matrix needed to sample densities\n",
    "    DesignMatrix = torch.cos(torch.pi * torch.outer(m, x))                  # (M, N)\n",
    "    DerDM = -torch.pi * m[:, None] * torch.sin(torch.pi * torch.outer(m, x))  # (M, N) # derivative of design matrix\n",
    "    std_harm = 2.0 / (1.0 + 0.2 * m)**2 # std_harm = 2.0 / (1.0 + m)**2\n",
    "elif data_regime == \"rough\":\n",
    "    M_cutoff = N_grid - 1 # maximum harmonic\n",
    "    m = torch.arange(1, M_cutoff+1, dtype=dtype, device=device)             # (M,)\n",
    "    x = torch.linspace(0, 1, N_grid, dtype=dtype, device=device)            # (N,)\n",
    "    #design matrix needed to sample densities\n",
    "    DesignMatrix = torch.cos(torch.pi * torch.outer(m, x))                  # (M, N)\n",
    "    DerDM = -torch.pi * m[:, None] * torch.sin(torch.pi * torch.outer(m, x))  # (M, N) # derivative of design matrix\n",
    "    std_harm = 2.0 / (1.0 + 0.0 * m)**2\n",
    "else:\n",
    "    raise ValueError(\"regime must be 'smooth' or 'rough'\")\n",
    "\n",
    "N_train = 1500\n",
    "N_test = 250\n",
    "N_val = 250\n",
    "\n",
    "N_batch = 50\n",
    "N_epochs = 10000\n",
    "lr = 1e-2 # we will use a LR scheduler, so this is just an initial value\n",
    "min_delta = 1e-5 # min change in the monitored quantity to qualify as an improvement\n",
    "patience = 30    # epochs to wait for improvement before stopping training'\n",
    "pad_mode = \"reflect\" # padding mode for convolution-based routines: either \"zero\" or \"reflect\"\n",
    "N_feat = 1 # number of local features per grid point\n",
    "\n",
    "# interaction kernel parameters\n",
    "kernel_regime = \"custom\"  # \"power\", \"yukawa\", or \"exp\", or \"custom\"\n",
    "\n",
    "lam = 10.0\n",
    "alpha = 1.0\n",
    "xi = 100.0\n",
    "amp_Gaussian_1 = -1.0 # amplitude of first Gaussian kernel for interaction energy\n",
    "sigma_Gaussian_1 = 3.0 # width of first Gaussian kernel for interaction energy\n",
    "amp_Gaussian_2 = 2.0 # amplitude of second Gaussian kernel for interaction energy\n",
    "sigma_Gaussian_2 = 1.0 # width of second Gaussian kernel for interaction energy\n",
    "\n",
    "if kernel_regime == \"power\":\n",
    "    def E_tot(rho: torch.Tensor) -> torch.Tensor:\n",
    "        return E_int_dct(rho, kernel=\"power\", alpha=alpha)\n",
    "elif kernel_regime == \"yukawa\": \n",
    "    def E_tot(rho: torch.Tensor) -> torch.Tensor:\n",
    "        return E_int_dct(rho, kernel=\"yukawa\", lam=lam)\n",
    "elif kernel_regime == \"exp\":\n",
    "    def E_tot(rho: torch.Tensor) -> torch.Tensor:\n",
    "        return E_int_dct(rho, kernel=\"exp\", xi=xi)\n",
    "elif kernel_regime == \"custom\":\n",
    "    def E_tot(rho: torch.Tensor) -> torch.Tensor:\n",
    "        return E_int_dct(rho, kernel=\"exp\", xi=xi) + \\\n",
    "                amp_Gaussian_1 * E_int_conv(rho, kernel=\"gaussian\", sigma=sigma_Gaussian_1, pad_mode=pad_mode) + \\\n",
    "                amp_Gaussian_2 * E_int_conv(rho, kernel=\"gaussian\", sigma=sigma_Gaussian_2, pad_mode=pad_mode)\n",
    "    \n",
    "R = 20 \n",
    "r_grid = torch.arange(-R, R+1)\n",
    "plt.figure(figsize=(6,4))\n",
    "if kernel_regime == \"yukawa\":\n",
    "    plt.plot(r_grid, K_yukawa(r_grid, lam=lam), 'o-', linewidth=2) \n",
    "elif kernel_regime == \"power\":\n",
    "    plt.plot(r_grid, K_power(r_grid, alpha=alpha), 'o-', linewidth=2) \n",
    "elif kernel_regime == \"exp\":\n",
    "    plt.plot(r_grid, K_exp(r_grid, xi=xi), 'o-', linewidth=2)\n",
    "elif kernel_regime == \"custom\":\n",
    "    K_r = K_exp(r_grid, xi=xi) + \\\n",
    "          amp_Gaussian_1 * K_gaussian(r_grid, sigma=sigma_Gaussian_1) + \\\n",
    "          amp_Gaussian_2 * K_gaussian(r_grid, sigma=sigma_Gaussian_2)\n",
    "    plt.plot(r_grid, K_r, 'o-', linewidth=2)\n",
    "plt.axhline(0, color='k', linewidth=0.5)\n",
    "plt.xlabel(\"r = i - j\")\n",
    "plt.ylabel(\"K_r\")\n",
    "plt.xlim(-R, R)\n",
    "# plt.ylim((0.0, 1.0))\n",
    "plt.title(f\"Real space kernel\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# generate train/test split\n",
    "rho_train, d_rho_train, a_train = sample_density_batch(N_train, std_harm=std_harm, DesignMatrix=DesignMatrix, DerDM=DerDM)  # (N_train, N_grid)\n",
    "rho_test, d_rho_test, a_test = sample_density_batch(N_test, std_harm=std_harm, DesignMatrix=DesignMatrix, DerDM=DerDM)   # (N_test, N_grid)\n",
    "rho_val, d_rho_val, a_val = sample_density_batch(N_val, std_harm=std_harm, DesignMatrix=DesignMatrix, DerDM=DerDM)    # (N_val, N_grid)\n",
    "features_train = generate_loc_features_rs(rho_train, N_feat=N_feat)  # (N_train, N_grid, N_feat)\n",
    "features_test  = generate_loc_features_rs(rho_test, N_feat=N_feat)   # (N_test, N_grid, N_feat)\n",
    "features_val   = generate_loc_features_rs(rho_val, N_feat=N_feat)    # (N_val, N_grid, N_feat)\n",
    "\n",
    "targets_train = E_tot(rho_train)            # (N_train,)\n",
    "targets_test  = E_tot(rho_test)             # (N_test,)\n",
    "targets_val   = E_tot(rho_val)              # (N_val,)\n",
    "\n",
    "# Normalize features\n",
    "mean_feat, std_feat = compute_normalization_stats(features_train)\n",
    "features_train_norm = normalize_features(features_train, mean_feat, std_feat)\n",
    "features_test_norm = normalize_features(features_test, mean_feat, std_feat)\n",
    "features_val_norm = normalize_features(features_val, mean_feat, std_feat)\n",
    "\n",
    "# Normalize targets\n",
    "E_mean = targets_train.mean()\n",
    "E_std = targets_train.std()\n",
    "targets_train_norm = (targets_train - E_mean) / E_std\n",
    "targets_test_norm = (targets_test - E_mean) / E_std\n",
    "targets_val_norm = (targets_val - E_mean) / E_std\n",
    "\n",
    "# Datasets\n",
    "train_dataset = TensorDataset(features_train_norm, targets_train_norm)\n",
    "val_dataset   = TensorDataset(features_val_norm,   targets_val_norm)\n",
    "test_dataset  = TensorDataset(features_test_norm,  targets_test_norm)\n",
    "\n",
    "# Loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=N_batch, shuffle=True,  drop_last=False)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=N_batch, shuffle=False, drop_last=False)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=N_batch, shuffle=False, drop_last=False)\n",
    "\n",
    "\n",
    "# For saving data used in Figure 2\n",
    "# N_batch = 50\n",
    "# rho_batch, _, a_batch = sample_density_batch(N_batch, std_harm=std_harm, DesignMatrix=DesignMatrix, DerDM=DerDM)\n",
    "# np.savetxt(\"Figure_2/rho_batch_\" + data_regime + \".txt\", rho_batch.cpu().numpy())\n",
    "# np.savetxt(\"Figure_2/a_batch_\" + data_regime + \".txt\", a_batch.cpu().numpy())\n",
    "# data = np.column_stack([m.cpu().numpy(), std_harm.cpu().numpy()])\n",
    "# np.savetxt(f\"Figure_2/harmonics_stats_\" + data_regime + \".txt\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc54438",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_dir = \"LearningNonLocalKernel_wRSDCTKernel_checkpoints\"\n",
    "flag_train = True  # set to True to train models\n",
    "learning_regime = \"hybrid\"  # \"dct_rs_blind\", \"dct_exp_rs_mixture\", \"dct_ms_blind\", or \"hybrid\"\n",
    "R = 10  # for hybrid\n",
    "n_exp_components = 1  # for hybrid\n",
    "\n",
    "if data_regime == \"smooth\":\n",
    "    range_ms = M_cutoff  # for dct_ms_blind\n",
    "else:\n",
    "    range_ms = N_grid - 1  # for dct_ms_blind\n",
    "\n",
    "if kernel_regime == \"exp\":\n",
    "            run_name = f\"rs_dct_kernel_\" + data_regime + '_' + kernel_regime + f\"_xi{xi}\" + learning_regime\n",
    "elif kernel_regime == \"yukawa\":\n",
    "    run_name = f\"rs_dct_kernel_\" + data_regime + '_' + kernel_regime + f\"_lam{lam}\" + learning_regime\n",
    "elif kernel_regime == \"power\":\n",
    "    run_name = f\"rs_dct_kernel_\" + data_regime + '_' + kernel_regime + f\"_alpha{alpha}\" + learning_regime\n",
    "elif kernel_regime == \"custom\":\n",
    "    run_name = f\"rs_dct_kernel_\" + data_regime + '_' + kernel_regime + '_' + learning_regime\n",
    "\n",
    "if flag_train:\n",
    "\n",
    "    torch.manual_seed(1234) # for reproducibility\n",
    "    \n",
    "    if learning_regime == \"dct_exp_rs_mixture\":\n",
    "        model = DCTKernelEnergyNN(\n",
    "            N=N_grid,\n",
    "            learning_mode=\"dct_exp_rs_mixture\",\n",
    "            n_components=1,\n",
    "            mean_feat=mean_feat,\n",
    "            std_feat=std_feat,\n",
    "            E_mean=E_mean,\n",
    "            E_std=E_std,\n",
    "        ).to(device)\n",
    "    elif learning_regime == \"dct_rs_blind\":\n",
    "        model = DCTKernelEnergyNN(\n",
    "            N=N_grid,\n",
    "            learning_mode=\"dct_rs_blind\",\n",
    "            zero_r_flag=False,   # or True if you want K_0=0\n",
    "            mean_feat=mean_feat,\n",
    "            std_feat=std_feat,\n",
    "            E_mean=E_mean,\n",
    "            E_std=E_std,\n",
    "        ).to(device)\n",
    "    elif learning_regime == \"dct_ms_blind\":\n",
    "        model = DCTKernelEnergyNN(\n",
    "            N=N_grid,\n",
    "            learning_mode=\"dct_ms_blind\",\n",
    "            range_ms=range_ms,\n",
    "            mean_feat=mean_feat,\n",
    "            std_feat=std_feat,\n",
    "            E_mean=E_mean,\n",
    "            E_std=E_std,\n",
    "        ).to(device)\n",
    "    elif learning_regime == \"hybrid\":\n",
    "        model = HybridKernelEnergyNN(\n",
    "            N=N_grid,\n",
    "            R=R,\n",
    "            n_exp_components=n_exp_components,\n",
    "            mean_feat=mean_feat,\n",
    "            std_feat=std_feat,\n",
    "            E_mean=E_mean,\n",
    "            E_std=E_std,\n",
    "        ).to(device)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown learning_regime: {learning_regime}\")\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Reduce LR when val loss plateaus\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=patience, cooldown=2, min_lr=1e-6\n",
    "    )\n",
    "\n",
    "    hist, best_epoch = train_with_early_stopping(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        max_epochs=N_epochs,\n",
    "        patience=patience,\n",
    "        min_delta=min_delta,\n",
    "        ckpt_dir=ckpt_dir,\n",
    "        run_name=run_name,\n",
    "        learning_regime=learning_regime,\n",
    "        N_grid=N_grid,\n",
    "        device=device,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a33478",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = ckpt_dir + f\"/{run_name}_history.csv\"\n",
    "hist_df = pd.read_csv(path)\n",
    "print(hist_df.head())\n",
    "hist_df.plot(x=\"epoch\", y=[\"train_loss\", \"val_loss\"], logy=True, grid=True, title=run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3fc477",
   "metadata": {},
   "outputs": [],
   "source": [
    "if learning_regime == \"hybrid\":\n",
    "    model, normalization, epoch, val_loss = load_checkpoint(\n",
    "        ckpt_dir + f\"/{run_name}_best.pt\",\n",
    "        HybridKernelEnergyNN,\n",
    "        device=device\n",
    "    )\n",
    "else:\n",
    "    model, normalization, epoch, val_loss = load_checkpoint(\n",
    "        ckpt_dir + f\"/{run_name}_best.pt\",\n",
    "        DCTKernelEnergyNN,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "if learning_regime == \"dct_exp_rs_mixture\":\n",
    "     print(\"Learned Exponential Mixture RS DCT Kernel parameters:\")\n",
    "     print(model.nonlocal_kernel.n_components)\n",
    "     print(\"Amplitudes A_n:\", model.nonlocal_kernel.amplitudes.detach().cpu().numpy())\n",
    "     print(\"Sigmas sigma_n:\", F.softplus(model.nonlocal_kernel.log_sigmas).detach().cpu().numpy())\n",
    "     with torch.no_grad():\n",
    "        k_full = model.nonlocal_kernel.build_kernel().detach().cpu().numpy()  # (N_grid,)\n",
    "elif learning_regime == \"dct_rs_blind\":\n",
    "    with torch.no_grad():\n",
    "        k_full = model.nonlocal_kernel.rs_kernel.detach().cpu().numpy()  # (N_grid,)\n",
    "elif learning_regime == \"dct_ms_blind\":\n",
    "    print(\"Learned MS DCT Kernel parameters:\")\n",
    "    print(\"Range m:\", model.nonlocal_kernel.range_ms)\n",
    "    with torch.no_grad():\n",
    "        lam_K = torch.zeros(N_grid, dtype=dtype)\n",
    "        lam_K[0] = 0.0\n",
    "        lam_K[1:1+range_ms] = model.nonlocal_kernel.ms_kernel.detach().cpu()\n",
    "        k_full = kernel_from_eigenvals_dct(lam_K).cpu().numpy()\n",
    "elif learning_regime == \"hybrid\":\n",
    "    print(\"Learned Hybrid Kernel parameters:\")\n",
    "    print(\"Local kernel range R:\", model.R)\n",
    "    print(\"Nonlocal exponential mixture components:\", model.n_exp_components)\n",
    "\n",
    "    print(\"Amplitudes A_n:\", model.nonlocal_kernel.amplitudes.detach().cpu().numpy())\n",
    "    print(\"Sigmas sigma_n:\", F.softplus(model.nonlocal_kernel.log_sigmas).detach().cpu().numpy())\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        k_local_small = model.local_kernel.build_kernel().squeeze()  # (2R+1,)\n",
    "        k_nonlocal = model.nonlocal_kernel.build_kernel()            # (N_grid,)\n",
    "\n",
    "        R = model.R  # local range\n",
    "        print(\"Local kernel values K_r (r=0..R):\", k_local_small[R:].cpu().numpy())\n",
    "\n",
    "        # take r = 0..R from symmetric kernel and pad zeros up to N_grid\n",
    "        # k_local_small layout: [-R, ..., -1, 0, 1, ..., R]\n",
    "        # index R is r=0, R+1..2R is r=1..R\n",
    "        k_local_full = torch.cat(\n",
    "            [\n",
    "                k_local_small[R:],  # (R+1,)  -> r=0..R\n",
    "                torch.zeros(N_grid - 1 - R, device=device, dtype=dtype),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )  # (N_grid,)\n",
    "        k_full = k_local_full.cpu().numpy() + k_nonlocal.cpu().numpy()  # (N_grid,)\n",
    "\n",
    "\n",
    "r_grid = np.arange(0, N_grid)\n",
    "\n",
    "if kernel_regime == \"exp\":\n",
    "    def K_true(r):\n",
    "        return np.exp(-np.abs(r) / xi)\n",
    "elif kernel_regime == \"yukawa\":\n",
    "    def K_true(r):\n",
    "        r_abs = np.abs(r)\n",
    "        out = np.exp(-r_abs / lam) / np.maximum(r_abs, 1.0)\n",
    "        return out * (r_abs > 0)\n",
    "elif kernel_regime == \"power\":\n",
    "    def K_true(r):\n",
    "        r_abs = np.abs(r)\n",
    "        out = 1.0 / (np.maximum(r_abs, 1.0)**alpha)\n",
    "        return out * (r_abs > 0)    \n",
    "elif kernel_regime == \"custom\":\n",
    "    def K_true(r):\n",
    "        r_abs = np.abs(r)\n",
    "        out = np.exp(-r_abs / xi) + \\\n",
    "              amp_Gaussian_1 * np.exp(- (r_abs**2) / (sigma_Gaussian_1**2)) + \\\n",
    "              amp_Gaussian_2 * np.exp(- (r_abs**2) / (sigma_Gaussian_2**2))\n",
    "        return out \n",
    "\n",
    "k_true = K_true(r_grid)\n",
    "lamb_K_true = kernel_eigenvals_dct(torch.tensor(k_true, dtype=dtype))\n",
    "lamb_K_true[0] = 0.0  # enforce zero mode\n",
    "k_true = kernel_from_eigenvals_dct(lamb_K_true).cpu().numpy()\n",
    "\n",
    "lamb_K_full = kernel_eigenvals_dct(torch.tensor(k_full, dtype=dtype))\n",
    "lamb_K_full[0] = 0.0  # enforce zero mode\n",
    "k_full = kernel_from_eigenvals_dct(lamb_K_full).cpu().numpy()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(r_grid, k_full , 'o-', label='Learned kernel (window)', linewidth=2)\n",
    "plt.plot(r_grid, k_true, 's--', label='True kernel', alpha=0.7)\n",
    "plt.axhline(0, color='k', linewidth=0.5)\n",
    "plt.xlabel(\"r = i - j\")\n",
    "plt.ylabel(\"K_r\")\n",
    "plt.xlim(0, 100)\n",
    "# plt.ylim((-0.01, 0.01))\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "if kernel_regime == \"exp\":\n",
    "    plt.title(f\"Local kernel learning ({data_regime}, exp, xi={xi})\")\n",
    "elif kernel_regime == \"yukawa\":\n",
    "    plt.title(f\"Local kernel learning ({data_regime}, yukawa, lam={lam})\")\n",
    "elif kernel_regime == \"power\":\n",
    "    plt.title(f\"Local kernel learning ({data_regime}, power, alpha={alpha})\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(r_grid, lamb_K_full , 'o-', label='Learned kernel (window)', linewidth=2)\n",
    "plt.plot(r_grid, lamb_K_true, 's--', label='True kernel', alpha=0.7)\n",
    "plt.axhline(0, color='k', linewidth=0.5)\n",
    "plt.xlabel(\"m\")\n",
    "plt.ylabel(\"lambda_m\")\n",
    "plt.xlim(0, 50)\n",
    "#plt.ylim((-0.05, 0.0))\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "if kernel_regime == \"exp\":\n",
    "    plt.title(f\"Local kernel learning ({data_regime}, exp, xi={xi})\")\n",
    "elif kernel_regime == \"yukawa\":\n",
    "    plt.title(f\"Local kernel learning ({data_regime}, yukawa, lam={lam})\")\n",
    "elif kernel_regime == \"power\":\n",
    "    plt.title(f\"Local kernel learning ({data_regime}, power, alpha={alpha})\")\n",
    "elif kernel_regime == \"custom\":\n",
    "    plt.title(f\"Local kernel learning ({data_regime}, custom kernel)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# For saving data used in Figure 2\n",
    "# data = np.column_stack([r_grid, k_full, k_true])\n",
    "# np.savetxt(f\"Figure_2/nonlocal_rs_kernel_xi{xi}\" + \"_\" + data_regime + \".txt\", data)\n",
    "# data = np.column_stack([r_grid, lamb_K_full, lamb_K_true])\n",
    "# np.savetxt(f\"Figure_2/nonlocal_ms_kernel_xi{xi}\" + \"_\" + data_regime + \".txt\", data)\n",
    "\n",
    "# data = np.column_stack([r_grid, k_full, k_true])\n",
    "# np.savetxt(f\"Figure_2/hybrid_nonlocal_rs_kernel_custom_R{R}_xi{xi}.txt\", data)\n",
    "# data = np.column_stack([r_grid, lamb_K_full, lamb_K_true])\n",
    "# np.savetxt(f\"Figure_2/hybrid_nonlocal_ms_kernel_custom_R{R}_xi{xi}.txt\", data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
