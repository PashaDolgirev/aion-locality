{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75ad941b",
   "metadata": {},
   "source": [
    "Total energy now contains nonlocal density-density interactions; we use the von Neumann boundary conditions (this ensures the interaction kernel is diagonal upon dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8622f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import math, os, time, copy\n",
    "import torch.fft as tfft\n",
    "import pandas as pd\n",
    "import torch_dct as dct\n",
    "from numpy import size\n",
    "\n",
    "torch.random.manual_seed(1234) # for reproducibility\n",
    "\n",
    "# Global settings\n",
    "dtype = torch.float64\n",
    "device = \"cpu\"\n",
    "\n",
    "data_regime = \"rough\" # \"smooth\" or \"rough\"\n",
    "\n",
    "N_grid = 512 # number of grid points\n",
    "if data_regime == \"smooth\":\n",
    "    M_cutoff = 50 # maximum harmonic\n",
    "    m = torch.arange(1, M_cutoff+1, dtype=dtype, device=device)             # (M,)\n",
    "    x = torch.linspace(0, 1, N_grid, dtype=dtype, device=device)            # (N,)\n",
    "    #design matrix needed to sample densities\n",
    "    DesignMatrix = torch.cos(torch.pi * torch.outer(m, x))                  # (M, N)\n",
    "    DerDM = -torch.pi * m[:, None] * torch.sin(torch.pi * torch.outer(m, x))  # (M, N) # derivative of design matrix\n",
    "    std_harm = 2.0 / (1.0 + m)**2\n",
    "elif data_regime == \"rough\":\n",
    "    M_cutoff = N_grid - 1 # maximum harmonic\n",
    "    m = torch.arange(1, M_cutoff+1, dtype=dtype, device=device)             # (M,)\n",
    "    x = torch.linspace(0, 1, N_grid, dtype=dtype, device=device)            # (N,)\n",
    "    #design matrix needed to sample densities\n",
    "    DesignMatrix = torch.cos(torch.pi * torch.outer(m, x))                  # (M, N)\n",
    "    DerDM = -torch.pi * m[:, None] * torch.sin(torch.pi * torch.outer(m, x))  # (M, N) # derivative of design matrix\n",
    "    std_harm = 2.0 / (1.0 + 0.0 * m)**2\n",
    "else:\n",
    "    raise ValueError(\"regime must be 'smooth' or 'rough'\")\n",
    "\n",
    "N_train = 1500\n",
    "N_test = 250\n",
    "N_val = 250\n",
    "\n",
    "N_batch = 50\n",
    "N_epochs = 10000\n",
    "lr = 1e-3 # we will use a LR scheduler, so this is just an initial value\n",
    "min_delta = 1e-5 # min change in the monitored quantity to qualify as an improvement\n",
    "patience = 30    # epochs to wait for improvement before stopping training'\n",
    "\n",
    "pad_mode = \"vonNeumann\" # padding mode for convolution-based routines: either \"zero\" or \"vonNeumann\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864ea1c4",
   "metadata": {},
   "source": [
    "Setup the functional and sample density profiles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9003c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_density(*, rho_b=0.0):\n",
    "    \"\"\"\n",
    "    Sample rho_j = rho_avg + sum_{m=1}^M a_m cos(m pi x_j), with x_j in [0,1]\n",
    "    this sampling of amplitudes a_m implies that the derivatives at the boundaries are zero\n",
    "    Sampling is done such that the generated density has zero mean over the grid points\n",
    "    Returns:\n",
    "      rho : (N,) density profile\n",
    "    \"\"\"    \n",
    "    a = torch.normal(torch.zeros_like(std_harm), std_harm)\n",
    "    rho = a @ DesignMatrix\n",
    "    d_rho_a = a @ DerDM  # derivative of rho w.r.t. x\n",
    "\n",
    "    return rho, d_rho_a, a\n",
    "\n",
    "def sample_density_batch(B: int, rho_b=0.0):\n",
    "    \"\"\"\n",
    "    Sample a batch of B density profiles\n",
    "    Spatial average density rho_avg is set to zero\n",
    "    Returns rho: (B, N_grid)\n",
    "    \"\"\"\n",
    "\n",
    "    a = torch.normal(torch.zeros(B, std_harm.numel(),dtype=dtype, device=device), std_harm.expand(B, -1))\n",
    "    rho = a @ DesignMatrix  # (B, N_grid)\n",
    "    d_rho_a = a @ DerDM  # (B, N_grid) # derivative of rho w.r.t. x\n",
    "\n",
    "    return rho, d_rho_a, a\n",
    "\n",
    "def rho_to_cosine_coeffs(rho_batch: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    rho_batch: (B, N) real tensor\n",
    "        rho[j] sampled on j = 0,...,N-1.\n",
    "    Returns:\n",
    "        a_batch: (B, N) cosine coefficients a_m in\n",
    "            rho_j = sum_m a_m cos(pi m j / (N-1)).\n",
    "    \"\"\"\n",
    "    # DCT-I along the last dimension\n",
    "    # torch_dct.dct1 works along the last axis by default\n",
    "    X = dct.dct1(rho_batch)          # shape (B, N)\n",
    "    B, N = X.shape\n",
    "\n",
    "    a = X.clone()\n",
    "    a[:, 0]      = 0.5 * X[:, 0]      / (N - 1)        # m = 0\n",
    "    a[:, 1:N-1]  = X[:, 1:N-1] / (N - 1)   # 1 <= m <= N-2\n",
    "    a[:, N-1]    = 0.5 * X[:, N-1]    / (N - 1)        # m = N-1\n",
    "\n",
    "    return a\n",
    "\n",
    "def cosine_coeffs_to_rho(a_batch: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Inverse of rho_to_cosine_coeffs, using inverse DCT-I.\n",
    "\n",
    "    a_batch: (B, N) of a_m\n",
    "    Returns:\n",
    "        rho_batch: (B, N)\n",
    "    \"\"\"\n",
    "    B, N = a_batch.shape\n",
    "\n",
    "    # Map back to standard DCT-I coefficients X_m\n",
    "    X = a_batch.clone()\n",
    "    X[:, 0]      = a_batch[:, 0]      * (N - 1) * 2.0\n",
    "    X[:, 1:N-1]  = a_batch[:, 1:N-1]  * (N - 1)\n",
    "    X[:, N-1]    = a_batch[:, N-1]    * (N - 1) * 2.0\n",
    "\n",
    "    # Inverse DCT-I along last dimension\n",
    "    rho_rec = dct.idct1(X)\n",
    "\n",
    "    return rho_rec\n",
    "\n",
    "# density–density interaction kernels K(r)\n",
    "# r: tensor (can be negative)\n",
    "def K_gaussian(r, sigma=1.0):       # strictly local-ish\n",
    "    r = r.to(dtype=dtype)\n",
    "    return torch.exp(-(r**2) / (sigma**2))\n",
    "\n",
    "def K_exp(r, xi=2.0):              # short–to–intermediate range\n",
    "    r = r.to(dtype=dtype)\n",
    "    return torch.exp(-torch.abs(r) / xi)\n",
    "\n",
    "def K_yukawa(r, lam=10.0):\n",
    "    r_abs = torch.abs(r).to(dtype=dtype)\n",
    "    out = torch.exp(-r_abs / lam) / r_abs.clamp(min=1.0)\n",
    "    return out * (r_abs > 0)     # zero at r == 0\n",
    "\n",
    "def K_power(r, alpha=1.0):         # unscreened long range (Coulomb-like for alpha=1)\n",
    "    r = torch.abs(r).to(dtype=dtype)\n",
    "    out = 1.0 / (r.clamp(min=1.0)**alpha)\n",
    "    return out * (r > 0)     # zero at r == 0\n",
    "\n",
    "def E_int_conv(rho: torch.Tensor, kernel: str, **kwargs) -> torch.Tensor: \n",
    "    \"\"\"\n",
    "    Interaction energy using convolution\n",
    "    rho: (N,) or (B, N)\n",
    "    kernel: \"gaussian\", \"exp\", \"yukawa\", \"power\"\n",
    "    kwargs: parameters for the kernel function (sigma, xi, lam, alpha, etc.)\n",
    "    Returns: scalar (if input 1D) or (B,) (if input 2D)\n",
    "    \"\"\"\n",
    "    if kernel == \"gaussian\":\n",
    "        K_fun = K_gaussian\n",
    "    elif kernel == \"exp\":\n",
    "        K_fun = K_exp\n",
    "    elif kernel == \"yukawa\":\n",
    "        K_fun = K_yukawa\n",
    "    elif kernel == \"power\":\n",
    "        K_fun = K_power\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown kernel: {kernel}\")\n",
    "\n",
    "    # ensure batch dim\n",
    "    if rho.dim() == 1:\n",
    "        rho = rho.unsqueeze(0)\n",
    "    B, N = rho.shape\n",
    "    device, dtype = rho.device, rho.dtype\n",
    "    r_vals = torch.arange(-(N-1), N, device=device, dtype=dtype)  # (2N-1,)\n",
    "    k_full = K_fun(r_vals, **kwargs)                              # (2N-1,)\n",
    "    weight = k_full.view(1, 1, -1)                # (1,1,2N-1)\n",
    "\n",
    "    if pad_mode == \"zero\":     \n",
    "        u = F.conv1d(rho.unsqueeze(1), weight, padding=N-1).squeeze(1) # (B, N)\n",
    "    elif pad_mode == \"vonNeumann\":\n",
    "        # even reflection padding\n",
    "        rho_pad = F.pad(rho.unsqueeze(1), (N-1, N-1), mode='reflect')   # (B,1,N+2R)\n",
    "        u = F.conv1d(rho_pad, weight).squeeze(1)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown padding: {pad_mode}\")\n",
    "    \n",
    "    E = 0.5 * (rho * u).sum(dim=-1) / N  # (B,)\n",
    "    return E.squeeze(0) if E.numel() == 1 else E\n",
    "\n",
    "def kernel_eigenvals_dct(K: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute the convolution eigenvalues for kernel K using DCT-I.\n",
    "\n",
    "    K: (N,) real tensor, kernel values at r = 0, 1, ..., N-1\n",
    "       (assumed even: K_r = K_{-r})\n",
    "\n",
    "    Returns:\n",
    "        lam_K: (N,) real tensor, eigenvalues λ_m of the convolution operator\n",
    "               with that kernel under even-reflection (Neumann BC).\n",
    "    \"\"\"\n",
    "    K = K.squeeze()\n",
    "    N = K.shape[-1]\n",
    "    X = dct.dct1(K)                     # (N,)\n",
    "\n",
    "    # λ_m = X_m + (-1)^m * K_{N-1}\n",
    "    lam_K = X + K[-1] * ((-1.0) ** torch.arange(N, device=K.device, dtype=K.dtype))     # (N,)\n",
    "    return lam_K\n",
    "\n",
    "def kernel_from_eigenvals_dct(lam_K: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Invert kernel_eigenvals_dct:\n",
    "    given eigenvalues lam_K (λ_m), reconstruct kernel K (K_r, r=0..N-1).\n",
    "\n",
    "    lam_K: (N,)\n",
    "    Returns:\n",
    "        K: (N,) real tensor, kernel values at r = 0, 1, ..., N-1\n",
    "    \"\"\"\n",
    "    \n",
    "    device, dtype = lam_K.device, lam_K.dtype\n",
    "    N = lam_K.shape[-1]\n",
    "\n",
    "    # v_m = (-1)^m\n",
    "    v = (-1.0) ** torch.arange(N, device=device, dtype=dtype)                # (N,)\n",
    "    A = dct.idct1(lam_K)           # (N,)\n",
    "    c = dct.idct1(v)               # (N,)\n",
    "\n",
    "    # K_{N-1} = A_{N-1} / (1 + c_{N-1})   (do this per batch)\n",
    "    denom = 1.0 + c[-1]\n",
    "    K_nm1 = A[-1] / denom       \n",
    "\n",
    "    # K = A - K_{N-1} * c\n",
    "    K = A - K_nm1 * c  \n",
    "    return K\n",
    "\n",
    "def E_int_dct(rho: torch.Tensor, kernel: str, **kwargs) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Interaction energy using DCT-I eigenvalues of the convolution operator\n",
    "    rho: (N,) or (B, N)\n",
    "    kernel: \"gaussian\", \"exp\", \"yukawa\", \"power\"\n",
    "    kwargs: parameters for the kernel function (sigma, xi, lam, alpha, etc.)\n",
    "    Returns: scalar (if input 1D) or (B,) (if input 2D)\n",
    "    \"\"\"\n",
    "    if kernel == \"gaussian\":\n",
    "        K_fun = K_gaussian\n",
    "    elif kernel == \"exp\":\n",
    "        K_fun = K_exp\n",
    "    elif kernel == \"yukawa\":\n",
    "        K_fun = K_yukawa\n",
    "    elif kernel == \"power\":\n",
    "        K_fun = K_power\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown kernel: {kernel}\")\n",
    "\n",
    "    # ensure batch dim\n",
    "    if rho.dim() == 1:\n",
    "        rho = rho.unsqueeze(0)\n",
    "    B, N = rho.shape\n",
    "    device, dtype = rho.device, rho.dtype\n",
    "    r_vals = torch.arange(0, N, device=device, dtype=dtype)  # (N,)\n",
    "    K_vals = K_fun(r_vals, **kwargs)                         # (N,)\n",
    "\n",
    "    lam_K = kernel_eigenvals_dct(K_vals).to(device=device, dtype=dtype) # (N,)\n",
    "    a = rho_to_cosine_coeffs(rho)        # (B, N)\n",
    "    u = cosine_coeffs_to_rho(lam_K.unsqueeze(0) * a)         # (B, N)\n",
    "    E = 0.5 * (rho * u).sum(dim=-1) / N  # (B,)\n",
    "    \n",
    "    return E.squeeze(0) if E.numel() == 1 else E\n",
    "\n",
    "def E_int_dct_v2(rho: torch.Tensor, kernel: str, **kwargs) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Interaction energy using DCT-I eigenvalues of the convolution operator\n",
    "    rho: (N,) or (B, N)\n",
    "    kernel: \"gaussian\", \"exp\", \"yukawa\", \"power\"\n",
    "    kwargs: parameters for the kernel function (sigma, xi, lam, alpha, etc.)\n",
    "    Returns: scalar (if input 1D) or (B,) (if input 2D)\n",
    "    \"\"\"\n",
    "    # select kernel\n",
    "    if kernel == \"gaussian\":\n",
    "        K_fun = K_gaussian\n",
    "    elif kernel == \"exp\":\n",
    "        K_fun = K_exp\n",
    "    elif kernel == \"yukawa\":\n",
    "        K_fun = K_yukawa\n",
    "    elif kernel == \"power\":\n",
    "        K_fun = K_power\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown kernel: {kernel}\")\n",
    "\n",
    "    if rho.dim() == 1:\n",
    "        rho = rho.unsqueeze(0)\n",
    "    B, N = rho.shape\n",
    "    device, dtype = rho.device, rho.dtype\n",
    "\n",
    "    r_vals = torch.arange(0, N, device=device, dtype=dtype)   # (N,)\n",
    "    K_vals = K_fun(r_vals, **kwargs)                          # (N,)\n",
    "    lam_K = kernel_eigenvals_dct(K_vals).to(device=device, dtype=dtype)  # (N,)\n",
    "    a = rho_to_cosine_coeffs(rho)                             # (B, N)\n",
    "\n",
    "    # spectral \"convolution\" coefficients: λ_m a_m\n",
    "    spec = lam_K.unsqueeze(0) * a                             # (B, N)\n",
    "\n",
    "    # ---------- diagonal (bulk) term ----------\n",
    "    # norms n_m = <φ_m|φ_m> in the weighted scalar product\n",
    "    norms = torch.full((N,), (N - 1.0) / 2.0,\n",
    "                       device=device, dtype=dtype)\n",
    "    norms[0] = N - 1.0\n",
    "    norms[-1] = N - 1.0\n",
    "\n",
    "    # E_diag = (1/(2N)) * sum_m λ_m a_m^2 n_m\n",
    "    E_diag = 0.5 * (spec * a * norms.unsqueeze(0)).sum(dim=-1) / N   # (B,)\n",
    "\n",
    "    # ---------- boundary (mixing) term ----------\n",
    "    # sum_m λ_m a_m\n",
    "    sum_spec = spec.sum(dim=-1)                                      # (B,)\n",
    "    # sum_m λ_m a_m (-1)^m\n",
    "    sign = (-1.0) ** torch.arange(N, device=device, dtype=dtype)     # (N,)\n",
    "    sum_spec_signed = (spec * sign.unsqueeze(0)).sum(dim=-1)         # (B,)\n",
    "\n",
    "    # E_bnd = (1/(4N)) * [ ρ_0 * sum_m λ_m a_m + ρ_{N-1} * sum_m λ_m a_m (-1)^m ]\n",
    "    E_bnd = 0.25 * (rho[:, 0] * sum_spec + rho[:, -1] * sum_spec_signed) / N  # (B,)\n",
    "\n",
    "    E = E_diag + E_bnd\n",
    "    return E.squeeze(0) if E.numel() == 1 else E\n",
    "\n",
    "lam = 10.0\n",
    "alpha = 1.0\n",
    "xi = 100.0\n",
    "kernel_regime = \"exp\"  \n",
    "\n",
    "if kernel_regime == \"power\":\n",
    "    def E_tot(rho: torch.Tensor) -> torch.Tensor:\n",
    "        return E_int_dct(rho, kernel=\"power\", alpha=alpha)\n",
    "elif kernel_regime == \"yukawa\": \n",
    "    def E_tot(rho: torch.Tensor) -> torch.Tensor:\n",
    "        return E_int_dct(rho, kernel=\"yukawa\", lam=lam)\n",
    "elif kernel_regime == \"exp\":\n",
    "    def E_tot(rho: torch.Tensor) -> torch.Tensor:\n",
    "        return E_int_dct(rho, kernel=\"exp\", xi=xi)\n",
    "\n",
    "R = 100 \n",
    "r_grid = torch.arange(-R, R+1)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "if kernel_regime == \"yukawa\":\n",
    "    plt.plot(r_grid, K_yukawa(r_grid, lam=lam), 'o-', linewidth=2) \n",
    "elif kernel_regime == \"power\":\n",
    "    plt.plot(r_grid, K_power(r_grid, alpha=alpha), 'o-', linewidth=2) \n",
    "elif kernel_regime == \"exp\":\n",
    "    plt.plot(r_grid, K_exp(r_grid, xi=xi), 'o-', linewidth=2)\n",
    "plt.axhline(0, color='k', linewidth=0.5)\n",
    "plt.xlabel(\"r = i - j\")\n",
    "plt.ylabel(\"K_r\")\n",
    "plt.xlim(-R, R)\n",
    "# plt.ylim((0.0, 1.0))\n",
    "plt.title(f\"Local kernel\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "rho_batch, d_rho_batch, a_batch = sample_density_batch(3)  # (B, N_grid)\n",
    "\n",
    "a_batch_rec = rho_to_cosine_coeffs(rho_batch)\n",
    "rho_batch_rec = cosine_coeffs_to_rho(a_batch_rec)\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(x.numpy(), rho_batch[0, :].numpy(), lw=2)\n",
    "plt.plot(x.numpy(), rho_batch_rec[0, :].numpy(), lw=2, ls='--')\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"rho(x)\")\n",
    "plt.title(\"Random sampled densities from cosine basis\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "vec = torch.arange(0, N_grid, dtype=dtype, device=device)\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(m.numpy(), a_batch[0, :].numpy(), lw=2)\n",
    "plt.plot(vec.numpy(), a_batch_rec[0, :].numpy(), lw=2, ls='--')\n",
    "plt.xlabel(\"m\")\n",
    "plt.ylabel(\"a_m\")\n",
    "plt.xlim(0, M_cutoff+5)\n",
    "plt.title(\"Random sampled densities from cosine basis\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "if data_regime == \"rough\":\n",
    "    print(\"Max abs diff in a_m:\", torch.max(torch.abs(a_batch - a_batch_rec[:, 1:])).item())\n",
    "else:\n",
    "    print(\"Max abs diff in a_m:\", torch.max(torch.abs(a_batch - a_batch_rec[:, 1:M_cutoff+1])).item())\n",
    "print(\"Max abs diff in rho:\", torch.max(torch.abs(rho_batch - rho_batch_rec)).item())\n",
    "print(\"Zero freq component (should be 0):\", a_batch_rec[:, 0])\n",
    "\n",
    "E_int_v1 = E_int_conv(rho_batch[0, :], kernel=\"power\", alpha=alpha)\n",
    "E_int_v2 = E_int_dct(rho_batch[0, :], kernel=\"power\", alpha=alpha)\n",
    "E_int_v3 = E_int_dct_v2(rho_batch[0, :], kernel=\"power\", alpha=alpha)\n",
    "print(f\"E_int_conv = {E_int_v1.item():.6f}, E_int_dct = {E_int_v2.item():.6f}, E_int_dct_v2 = {E_int_v3.item():.6f}, diff = {abs(E_int_v1 - E_int_v2).item()}, diff_v2 = {abs(E_int_v1 - E_int_v3).item()}\")\n",
    "\n",
    "#Testing kernel eigenvalues and kernel reconstruction\n",
    "r_vals = torch.arange(0, N_grid, device=device, dtype=dtype)\n",
    "if kernel_regime == \"power\":\n",
    "    K_vals = K_power(r_vals, alpha=alpha)\n",
    "elif kernel_regime == \"yukawa\":\n",
    "    K_vals = K_yukawa(r_vals, lam=lam)\n",
    "elif kernel_regime == \"exp\":\n",
    "    K_vals = K_exp(r_vals, xi=xi)\n",
    "\n",
    "lam_K = kernel_eigenvals_dct(K_vals)\n",
    "K_rec = kernel_from_eigenvals_dct(lam_K)\n",
    "\n",
    "print(\"Max abs diff in kernel reconstruction:\", torch.max(torch.abs(K_vals - K_rec)).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddb6d68",
   "metadata": {},
   "source": [
    "Feature processing (generation and normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e5f54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we save features as (B, N_grid, N_feat), where N_feat is the number of features per grid point\n",
    "# generate train/test split\n",
    "\n",
    "def compute_normalization_stats(features):\n",
    "    \"\"\"\n",
    "    Compute mean and std for features with shape (N_data, N_grid, N_feat)\n",
    "    Averages over both data and spatial dimensions\n",
    "    \n",
    "    Args:\n",
    "        features: torch.Tensor of shape (N_data, N_grid, N_feat)\n",
    "    \n",
    "    Returns:\n",
    "        mean: torch.Tensor of shape (1, 1, N_feat)\n",
    "        std: torch.Tensor of shape (1, 1, N_feat)\n",
    "    \"\"\"\n",
    "\n",
    "    mean_feat = features.mean(dim=(0, 1), keepdim=True)  # Shape: (1, 1, N_feat)\n",
    "    std_feat = features.std(dim=(0, 1), keepdim=True) # Shape: (1, 1, N_feat)\n",
    "    \n",
    "    return mean_feat, std_feat\n",
    "\n",
    "def normalize_features(features, mean_feat, std_feat):\n",
    "    \"\"\"\n",
    "    Normalize features using provided or computed statistics\n",
    "    \n",
    "    Args:\n",
    "        features: torch.Tensor of shape (B, N_grid, N_feat)\n",
    "        mean: torch.Tensor of shape (1, 1, N_feat)\n",
    "        std: torch.Tensor of shape (1, 1, N_feat)\n",
    "\n",
    "    Returns:\n",
    "        normalized_features: torch.Tensor of same shape as input\n",
    "        mean: mean used for normalization\n",
    "        std: std used for normalization\n",
    "    \"\"\"\n",
    "    normalized_features = (features - mean_feat) / std_feat\n",
    "\n",
    "    return normalized_features\n",
    "\n",
    "def generate_loc_features_rs(rho: torch.Tensor, N_feat=2) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Generate local features from density rho\n",
    "    rs, real space   \n",
    "    Args:\n",
    "        rho: torch.Tensor of shape (B, N_grid)\n",
    "        N_feat: int, number of features to generate\n",
    "\n",
    "    Returns:\n",
    "        features: torch.Tensor of shape (B, N_grid, N_feat)\n",
    "        each feature is of the form rho^k, k=1,...,N_feat\n",
    "    \"\"\"\n",
    "    features = [rho.unsqueeze(-1) ** k for k in range(1, N_feat + 1)]\n",
    "    return torch.cat(features, dim=-1)\n",
    "\n",
    "def generate_loc_features_ms(d_rho: torch.Tensor, N_feat=2) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Generate local features from density derivative d_rho\n",
    "    ms, momentum space\n",
    "    Args:\n",
    "        d_rho: torch.Tensor of shape (B, N_grid)\n",
    "        N_feat: int, number of features to generate\n",
    "\n",
    "    Returns:\n",
    "        features: torch.Tensor of shape (B, N_grid, N_feat)\n",
    "        each feature is of the form d_rho^k, k=1,...,N_feat\n",
    "    \"\"\"\n",
    "    features = [d_rho.unsqueeze(-1) ** k for k in range(1, N_feat + 1)]\n",
    "    return torch.cat(features, dim=-1)\n",
    "\n",
    "rho_train, d_rho_train, a_train = sample_density_batch(N_train)  # (N_train, N_grid)\n",
    "rho_test, d_rho_test, a_test = sample_density_batch(N_test)   # (N_test, N_grid)\n",
    "rho_val, d_rho_val, a_val = sample_density_batch(N_val)    # (N_val, N_grid)\n",
    "\n",
    "N_feat = 1 \n",
    "\n",
    "features_train_rs = generate_loc_features_rs(rho_train, N_feat=N_feat)  # (N_train, N_grid, N_feat)\n",
    "features_test_rs  = generate_loc_features_rs(rho_test, N_feat=N_feat)   # (N_test, N_grid, N_feat)\n",
    "features_val_rs   = generate_loc_features_rs(rho_val, N_feat=N_feat)    # (N_val, N_grid, N_feat)\n",
    "\n",
    "features_train_ms = generate_loc_features_ms(d_rho_train, N_feat=N_feat)  # (N_train, N_grid, N_feat)\n",
    "features_test_ms  = generate_loc_features_ms(d_rho_test, N_feat=N_feat)   # (N_test, N_grid, N_feat)\n",
    "features_val_ms   = generate_loc_features_ms(d_rho_val, N_feat=N_feat)    # (N_val, N_grid, N_feat)\n",
    "\n",
    "features_train = torch.cat([features_train_rs, features_train_ms], dim=-1)\n",
    "features_test  = torch.cat([features_test_rs, features_test_ms], dim=-1)\n",
    "features_val   = torch.cat([features_val_rs, features_val_ms], dim=-1)\n",
    "\n",
    "targets_train = E_tot(rho_train)            # (N_train,)\n",
    "targets_test  = E_tot(rho_test)             # (N_test,)\n",
    "targets_val   = E_tot(rho_val)              # (N_val,)\n",
    "\n",
    "# Normalize features\n",
    "mean_feat, std_feat = compute_normalization_stats(features_train)\n",
    "features_train_norm = normalize_features(features_train, mean_feat, std_feat)\n",
    "features_test_norm = normalize_features(features_test, mean_feat, std_feat)\n",
    "features_val_norm = normalize_features(features_val, mean_feat, std_feat)\n",
    "\n",
    "# Normalize targets\n",
    "E_mean = targets_train.mean()\n",
    "E_std = targets_train.std()\n",
    "targets_train_norm = (targets_train - E_mean) / E_std\n",
    "targets_test_norm = (targets_test - E_mean) / E_std\n",
    "targets_val_norm = (targets_val - E_mean) / E_std\n",
    "\n",
    "# Datasets\n",
    "train_dataset = TensorDataset(features_train_norm, targets_train_norm)\n",
    "val_dataset   = TensorDataset(features_val_norm,   targets_val_norm)\n",
    "test_dataset  = TensorDataset(features_test_norm,  targets_test_norm)\n",
    "\n",
    "# Loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=N_batch, shuffle=True,  drop_last=False)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=N_batch, shuffle=False, drop_last=False)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=N_batch, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f741e59f",
   "metadata": {},
   "source": [
    "Learning models: \n",
    "1. A real space model of a local interaction kernel kernel (either a window or a mixture of Guassians)\n",
    "2. A momentum space model via DCT-I (we either parametrize the interaction kernel in momentum space or real space, the parametrization can be a model with a few learnable parameters)\n",
    "3. Hybrid where the short-range part is captured via the real space approach, while the long-range part via the momentum space one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc323535",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearnableKernelConv1d(nn.Module):\n",
    "    \"\"\"\n",
    "    Learnable 1D convolution kernel K_r with range R\n",
    "    Produces phi = [K * rho] (linear convolution with padding)\n",
    "    \"\"\"\n",
    "    def __init__(self, R=5, even_kernel=True, pad_mode=\"zero\"):\n",
    "        super().__init__()\n",
    "        self.R = R\n",
    "        self.pad_mode = pad_mode\n",
    "        self.even_kernel = even_kernel\n",
    "        \n",
    "        if even_kernel:\n",
    "            # learn half + center: w[0] (center), w[1..R] (positive r)\n",
    "            self.kernel_half = nn.Parameter(torch.randn(R+1) * 0.01)\n",
    "        else:\n",
    "            # fully unconstrained kernel of size 2R+1\n",
    "            self.kernel = nn.Parameter(torch.randn(2*R+1) * 0.01)\n",
    "\n",
    "    def build_kernel(self):\n",
    "        \"\"\"\n",
    "        Returns kernel of shape (1,1,2R+1) as required by conv1d\n",
    "        \"\"\"\n",
    "        if self.even_kernel:\n",
    "            center = self.kernel_half[0:1]          # (1,)\n",
    "            pos = self.kernel_half[1:]             # (R,)\n",
    "            neg = pos.flip(0)               # symmetric\n",
    "            full = torch.cat([neg, center, pos], dim=0)  # (2R+1,)\n",
    "        else:\n",
    "            full = self.kernel\n",
    "        return full.view(1,1,-1)  # (out=1, in=1, kernel_size)\n",
    "\n",
    "    def forward(self, rho):\n",
    "        \"\"\"\n",
    "        rho: (B, N_grid)\n",
    "        Returns: phi: (B, N_grid)\n",
    "        \"\"\"\n",
    "        B, N = rho.shape\n",
    "        kernel = self.build_kernel()\n",
    "        kernel = kernel.to(dtype=rho.dtype, device=rho.device)\n",
    "        R = self.R\n",
    "        \n",
    "        if self.pad_mode == \"zero\":\n",
    "            x = F.pad(rho.unsqueeze(1), (R, R), mode='constant', value=0.0)\n",
    "        elif self.pad_mode == \"reflect\":\n",
    "            x = F.pad(rho.unsqueeze(1), (R, R), mode='reflect')\n",
    "        else:\n",
    "            raise ValueError(\"pad_mode must be zero or reflect\")\n",
    "        \n",
    "        phi = F.conv1d(x, kernel, padding=0).squeeze(1)\n",
    "        return phi\n",
    "\n",
    "class KernelOnlyEnergyNN(nn.Module):\n",
    "    \"\"\"\n",
    "    E_tot = (1 / 2N) * sum_{i,j} rho_i rho_j K_{i-j} = (1 / 2N) * sum_{i} rho_i [K * rho]_i\n",
    "    The kernel K is assumed local and learnable via convolution\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, R=5):\n",
    "        super().__init__()\n",
    "        self.R = R\n",
    "        self.kernel_conv = LearnableKernelConv1d(R, even_kernel=True, pad_mode=\"zero\")\n",
    "\n",
    "    def forward(self, features):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features: (B, N_grid, N_feat) - only the first feature (density) is used\n",
    "        \n",
    "        Returns:\n",
    "            local_energies: (B, N_grid) - energy at each grid point\n",
    "            total_energy: (B,) - sum over grid points\n",
    "        \"\"\"\n",
    "        rho_norm = features[..., 0]          # (B, N_grid), isolate density\n",
    "        rho = rho_norm * std_feat[0,0,0] + mean_feat[0,0,0]  # denormalize density\n",
    "\n",
    "        # apply learnable convolution kernel\n",
    "        phi = self.kernel_conv(rho)  # (B, N_grid) # in physical units\n",
    "\n",
    "        local_energies = 0.5 * rho * phi       # (B, N_grid) # physical units\n",
    "        total_energy = local_energies.sum(dim=1) / N_grid       # (B,) # physical units\n",
    "        \n",
    "        total_energy_norm = (total_energy - E_mean) / E_std  # normalize for training\n",
    "\n",
    "        return total_energy_norm\n",
    "    \n",
    "class GaussianMixtureKernelConv1d(nn.Module):\n",
    "    \"\"\"\n",
    "    Learnable 1D kernel K_r represented as a sum of Gaussians:\n",
    "        K(r) = sum_{n=1}^M A_n * exp(-r^2 / sigma_n^2)\n",
    "\n",
    "    Produces phi = [K * rho] via conv1d.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, R: int, n_components: int, pad_mode: str = \"zero\"):\n",
    "        super().__init__()\n",
    "        self.R = R\n",
    "        self.n_components = n_components\n",
    "        self.pad_mode = pad_mode\n",
    "\n",
    "        # r-grid as a buffer: [-R, ..., R] - still is a cutoff in real space\n",
    "        r_vals = torch.arange(-R, R + 1, dtype=dtype)\n",
    "        self.register_buffer(\"r_vals\", r_vals)  # (2R+1,)\n",
    "\n",
    "        # ---- amplitudes A_n (sorted at init) ----\n",
    "        # sample, then sort descending so A_1 >= A_2 >= ... >= A_M\n",
    "        amps = 0.01 * torch.randn(n_components, dtype=dtype)\n",
    "        amps, _ = torch.sort(amps, descending=True)\n",
    "        self.amplitudes = nn.Parameter(amps)\n",
    "\n",
    "        # Log-sigmas so sigma_n = softplus(log_sigma_n) > 0\n",
    "        init_sigmas = torch.arange(1, n_components+1, dtype=dtype)**2\n",
    "        log_sigmas = torch.log(torch.expm1(init_sigmas))  # inverse softplus, so softplus(raw)=init_sigma\n",
    "        self.log_sigmas = nn.Parameter(log_sigmas)\n",
    "\n",
    "    def build_kernel(self):\n",
    "        \"\"\"\n",
    "        Returns kernel of shape (1, 1, 2R+1) as required by conv1d.\n",
    "        \"\"\"\n",
    "        # (2R+1,) -> (1, L)\n",
    "        r = self.r_vals.view(1, -1)              # (1, 2R+1)\n",
    "        r2 = r * r                               # r^2\n",
    "\n",
    "        sigmas = F.softplus(self.log_sigmas) + 1e-8   # (M,)\n",
    "        sigma2 = sigmas.view(-1, 1) ** 2              # (M,1)\n",
    "\n",
    "        # contributions from each Gaussian: (M, L)\n",
    "        # exp(-r^2 / sigma_n^2)\n",
    "        gauss = torch.exp(-r2 / sigma2)          # (M, 2R+1)\n",
    "\n",
    "        # weighted sum over components\n",
    "        kernel_1d = (self.amplitudes.view(-1, 1) * gauss).sum(dim=0)  # (2R+1,)\n",
    "\n",
    "        # conv1d expects (out_channels, in_channels, kernel_size)\n",
    "        return kernel_1d.view(1, 1, -1)\n",
    "\n",
    "    def forward(self, rho: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        rho: (B, N_grid)\n",
    "        Returns: phi = (K * rho): (B, N_grid)\n",
    "        \"\"\"\n",
    "        B, N = rho.shape\n",
    "        kernel = self.build_kernel()\n",
    "        R = self.R\n",
    "\n",
    "        if self.pad_mode == \"zero\":\n",
    "            x = F.pad(rho.unsqueeze(1), (R, R), mode=\"constant\", value=0.0)\n",
    "        elif self.pad_mode == \"reflect\":\n",
    "            x = F.pad(rho.unsqueeze(1), (R, R), mode=\"reflect\")\n",
    "        else:\n",
    "            raise ValueError(\"pad_mode must be 'zero' or 'reflect'\")\n",
    "\n",
    "        phi = F.conv1d(x, kernel, padding=0).squeeze(1)   # (B, N)\n",
    "        return phi\n",
    "\n",
    "\n",
    "class GaussianMixtureEnergyNN(nn.Module):\n",
    "    \"\"\"\n",
    "    E_tot = (1 / 2N) * sum_{i,j} rho_i rho_j K_{i-j}\n",
    "          = (1 / 2N) * sum_i rho_i [K * rho]_i\n",
    "\n",
    "    K is parameterized as a sum of Gaussians in r.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, R=20, n_components=3, pad_mode=\"zero\"):\n",
    "        super().__init__()\n",
    "        self.R = R\n",
    "        self.kernel_conv = GaussianMixtureKernelConv1d(\n",
    "            R=R,\n",
    "            n_components=n_components,\n",
    "            pad_mode=pad_mode,\n",
    "        )\n",
    "\n",
    "    def forward(self, features):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features: (B, N_grid, N_feat) - only the first feature (density) is used\n",
    "\n",
    "        Returns:\n",
    "            total_energy_norm: (B,) - normalized total energy\n",
    "        \"\"\"\n",
    "        # globals: std_feat, mean_feat, E_mean, E_std, N_grid must exist\n",
    "        rho_norm = features[..., 0]          # (B, N_grid)\n",
    "        rho = rho_norm * std_feat[0, 0, 0] + mean_feat[0, 0, 0]\n",
    "\n",
    "        # apply Gaussian-mixture kernel\n",
    "        phi = self.kernel_conv(rho)          # (B, N_grid)\n",
    "\n",
    "        local_energies = 0.5 * rho * phi     # (B, N_grid)\n",
    "        total_energy = local_energies.sum(dim=1) / N_grid    # (B,)\n",
    "\n",
    "        total_energy_norm = (total_energy - E_mean) / E_std\n",
    "        return total_energy_norm\n",
    "    \n",
    "class LearnableRSNonLocalKernelDCT(nn.Module):\n",
    "    \"\"\"\n",
    "    Learnable 1D nonlocal kernel K_r with full range N_grid\n",
    "    Produces phi = [K * rho] via DCT-I\n",
    "    von Neumann BCs (even reflection)\n",
    "    RS = real space\n",
    "    \"\"\"\n",
    "    def __init__(self, zero_r_flag=True):\n",
    "        super().__init__()\n",
    "        self.zero_r_flag = zero_r_flag\n",
    "        if zero_r_flag:\n",
    "            self.rs_kernel = nn.Parameter(torch.randn(N_grid) * 0.01)  # full kernel K_r, r=0..N_grid-1\n",
    "            self.rs_kernel.data[0] = 0.0    # enforce K_0 = 0\n",
    "        else:\n",
    "            self.rs_kernel = nn.Parameter(torch.randn(N_grid) * 0.01) # full kernel K_r, r=0..N_grid-1\n",
    "\n",
    "    def forward(self, rho):\n",
    "        \"\"\"\n",
    "        rho: (B, N_grid)\n",
    "        Returns: phi: (B, N_grid)\n",
    "        \"\"\"\n",
    "        kernel = self.rs_kernel.to(dtype=rho.dtype, device=rho.device) # (N_grid,)\n",
    "        if self.zero_r_flag:\n",
    "            kernel = kernel.clone()\n",
    "            kernel[..., 0] = 0.0\n",
    "\n",
    "        lam_K = kernel_eigenvals_dct(kernel).to(device=rho.device, dtype=rho.dtype) # (N_grid,)\n",
    "        a = rho_to_cosine_coeffs(rho)\n",
    "        phi = cosine_coeffs_to_rho(lam_K.unsqueeze(0) * a)\n",
    "\n",
    "        return phi\n",
    "\n",
    "class ExpMixtureRSNonLocalKernelDCT(nn.Module):\n",
    "    \"\"\"\n",
    "    Learnable 1D kernel K_r represented as a sum of exponentials:\n",
    "        K(r) = sum_{n=1}^M A_n * exp(-r / sigma_n)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, zero_r_flag=True, n_components=3, N=N_grid): # N = grid size\n",
    "        super().__init__()\n",
    "        self.zero_r_flag = zero_r_flag\n",
    "        self.n_components = n_components\n",
    "        self.N = N\n",
    "\n",
    "        r_vals = torch.arange(0, N, dtype=dtype)\n",
    "        self.register_buffer(\"r_vals\", r_vals)  # (N,)\n",
    "\n",
    "        # ---- amplitudes A_n (sorted at init) ----\n",
    "        # sample, then sort descending so A_1 >= A_2 >= ... >= A_M\n",
    "        amps = 0.01 * torch.randn(n_components, dtype=dtype)\n",
    "        amps, _ = torch.sort(amps, descending=True)\n",
    "        self.amplitudes = nn.Parameter(amps)\n",
    "\n",
    "        # Log-sigmas so sigma_n = softplus(log_sigma_n) > 0\n",
    "        init_sigmas = 10 + 20 * torch.arange(n_components, dtype=dtype)\n",
    "        log_sigmas = torch.log(torch.expm1(init_sigmas))  # inverse softplus, so softplus(raw)=init_sigma\n",
    "        self.log_sigmas = nn.Parameter(log_sigmas)\n",
    "\n",
    "    def build_kernel(self):\n",
    "        r = self.r_vals.view(1, -1)              # (1, N)\n",
    "        sigmas = F.softplus(self.log_sigmas) + 1e-8   # (M,)\n",
    "        exp_mixt = torch.exp(-r / sigmas.view(-1, 1))          # (M, N)\n",
    "        return (self.amplitudes.view(-1, 1) * exp_mixt).sum(dim=0)  # (N,)\n",
    "\n",
    "    def forward(self, rho: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        rho: (B, N_grid)\n",
    "        Returns: phi = (K * rho): (B, N_grid)\n",
    "        \"\"\"\n",
    "        kernel = self.build_kernel()\n",
    "        if self.zero_r_flag:\n",
    "            kernel = kernel.clone()\n",
    "            kernel[..., 0] = 0.0\n",
    "\n",
    "        lam_K = kernel_eigenvals_dct(kernel).to(device=rho.device, dtype=rho.dtype) # (N_grid,)\n",
    "        a = rho_to_cosine_coeffs(rho)\n",
    "        return cosine_coeffs_to_rho(lam_K.unsqueeze(0) * a)\n",
    "    \n",
    "class LearnableMSNonLocalKernelDCT(nn.Module):\n",
    "    \"\"\"\n",
    "    Learnable nonlocal kernel parameterized directly in momentum space\n",
    "\n",
    "    We learn λ_m for m = 1..range_ms.\n",
    "    λ_0 is set to 0 (no uniform component), and λ_m = 0 for m > range_ms.\n",
    "    \"\"\"\n",
    "    def __init__(self, range_ms: int = 50):\n",
    "        super().__init__()\n",
    "        self.range_ms = range_ms\n",
    "        # learnable λ_m for m=1..range_ms\n",
    "        self.ms_kernel = nn.Parameter(torch.randn(range_ms) * 0.01)\n",
    "\n",
    "    def forward(self, rho: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        rho: (B, N_grid)\n",
    "        Returns: phi = (K * rho): (B, N_grid)\n",
    "        \"\"\"\n",
    "        B, N_grid = rho.shape\n",
    "        device, dtype = rho.device, rho.dtype\n",
    "\n",
    "        lam_head0 = torch.zeros(1, device=device, dtype=dtype)  \n",
    "        lam_active = self.ms_kernel.to(device=device, dtype=dtype)  \n",
    "        lam_tail = torch.zeros(N_grid - 1 - self.range_ms, device=device, dtype=dtype)\n",
    "        lam_K = torch.cat([lam_head0, lam_active, lam_tail], dim=0)  # (N_grid,)\n",
    "\n",
    "        a = rho_to_cosine_coeffs(rho)                      # (B, N_grid)\n",
    "        return cosine_coeffs_to_rho(lam_K.unsqueeze(0) * a) # (B, N_grid)\n",
    "    \n",
    "class DCTKernelEnergyNN(nn.Module):\n",
    "    \"\"\"\n",
    "    E_tot = (1 / 2N) sum_i rho_i [K * rho]_i\n",
    "    where K is represented via either:\n",
    "            (i) its DCT-I eigenvalues λ_m, which are learnable parameters,\n",
    "            (ii) via a real-space kernel K_r (K_-r = K_r) but computations are done via DCT-I (speedup N log N vs N^2),\n",
    "            (iii) via a mixture of learnable functions (with a few parameters), either in real space or in DCT-I space.\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_mode='dct_rs_blind', zero_r_flag=False, n_components=3, range_ms=50):\n",
    "        super().__init__()\n",
    "        self.learning_mode = learning_mode\n",
    "        if learning_mode == 'dct_rs_blind':\n",
    "            self.nonlocal_kernel = LearnableRSNonLocalKernelDCT(zero_r_flag=zero_r_flag)  # zero_r_flag=True enforces K_0 = 0\n",
    "        elif learning_mode == 'dct_exp_rs_mixture':\n",
    "            self.nonlocal_kernel = ExpMixtureRSNonLocalKernelDCT(n_components=n_components, N=N_grid, zero_r_flag=zero_r_flag)\n",
    "        elif learning_mode == 'dct_ms_blind':\n",
    "            self.nonlocal_kernel = LearnableMSNonLocalKernelDCT(range_ms=range_ms)\n",
    "    \n",
    "    def forward(self, features):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features: (B, N_grid, N_feat) - only the first feature (density) is used\n",
    "\n",
    "        Returns:\n",
    "            total_energy_norm: (B,) - normalized total energy\n",
    "        \"\"\"\n",
    "        # globals: std_feat, mean_feat, E_mean, E_std, N_grid must exist\n",
    "        rho_norm = features[..., 0]          # (B, N_grid)\n",
    "        rho = rho_norm * std_feat[0, 0, 0] + mean_feat[0, 0, 0]\n",
    "\n",
    "        phi = self.nonlocal_kernel(rho)          # (B, N_grid)\n",
    "\n",
    "        local_energies = 0.5 * rho * phi     # (B, N_grid)\n",
    "        total_energy = local_energies.sum(dim=1) / N_grid    # (B,)\n",
    "\n",
    "        total_energy_norm = (total_energy - E_mean) / E_std\n",
    "        return total_energy_norm\n",
    "    \n",
    "\n",
    "class HybridKernelEnergyNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Hybrid local + nonlocal kernel energy model\n",
    "    E_tot = E_local + E_nonlocal\n",
    "    \"\"\"\n",
    "    def __init__(self, R=5, n_exp_components=1):\n",
    "        super().__init__()\n",
    "        self.R = R\n",
    "        self.n_exp_components = n_exp_components\n",
    "\n",
    "        self.local_kernel = LearnableKernelConv1d(R=R, even_kernel=True, pad_mode=\"reflect\")\n",
    "        self.nonlocal_kernel = ExpMixtureRSNonLocalKernelDCT(n_components=n_exp_components, N=N_grid, zero_r_flag=False)\n",
    "\n",
    "    def forward(self, features):\n",
    "        rho_norm = features[..., 0]          # (B, N_grid)\n",
    "        rho = rho_norm * std_feat[0, 0, 0] + mean_feat[0, 0, 0]\n",
    "\n",
    "        phi_local = self.local_kernel(rho)          # (B, N_grid)\n",
    "        phi_nonlocal = self.nonlocal_kernel(rho)          # (B, N_grid)\n",
    "        E_local = 0.5 * (rho * phi_local).sum(dim=1) / N_grid       # (B,)\n",
    "        E_nonlocal = 0.5 * (rho * phi_nonlocal).sum(dim=1) / N_grid   # (B,)\n",
    "        total_energy = E_local + E_nonlocal\n",
    "\n",
    "        total_energy_norm = (total_energy - E_mean) / E_std\n",
    "        return total_energy_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020745f5",
   "metadata": {},
   "source": [
    "Intermediate routines for training + actual model exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc54438",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    n_batches = 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            y_pred = model(xb)\n",
    "            loss = criterion(y_pred, yb)\n",
    "            total_loss += loss.item()\n",
    "            n_batches += 1\n",
    "    return total_loss / max(1, n_batches)\n",
    "\n",
    "def load_checkpoint(path, model_class, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Loads a saved model checkpoint\n",
    "    Returns:\n",
    "        model: reconstructed and loaded model\n",
    "        normalization: dict of normalization stats\n",
    "        epoch: best epoch\n",
    "        val_loss: best validation loss\n",
    "    \"\"\"\n",
    "    ckpt = torch.load(path, map_location=device)\n",
    "    config = ckpt[\"config\"]\n",
    "    model = model_class(**config).to(device)\n",
    "    model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "    model.eval()\n",
    "\n",
    "    normalization = ckpt.get(\"normalization\", None)\n",
    "    epoch = ckpt.get(\"epoch\", None)\n",
    "    val_loss = ckpt.get(\"val_loss\", None)\n",
    "    return model, normalization, epoch, val_loss\n",
    "    \n",
    "def _run_epoch(model, loader, criterion, train: bool):\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "    running = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    with torch.set_grad_enabled(train):\n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "\n",
    "            if train:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            total_pred = model(xb)               \n",
    "            loss = criterion(total_pred, yb)        \n",
    "\n",
    "            if train:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            running += loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "    return running / max(1, n_batches)\n",
    "\n",
    "def train_with_early_stopping(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    criterion,\n",
    "    scheduler=None,\n",
    "    max_epochs=10000,\n",
    "    patience=10,\n",
    "    min_delta=1e-5,\n",
    "    ckpt_dir=\"checkpoints\",\n",
    "    run_name=None,\n",
    "    learning_regime=\"window\",\n",
    "):\n",
    "    os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "    best_val = math.inf\n",
    "    best_state = None\n",
    "    best_epoch = -1\n",
    "    since_improved = 0\n",
    "\n",
    "    hist = {\"train_loss\": [], \"val_loss\": []}\n",
    "\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        train_loss = _run_epoch(model, train_loader, criterion, train=True)\n",
    "        val_loss = _run_epoch(model, val_loader, criterion, train=False)\n",
    "\n",
    "        hist[\"train_loss\"].append(train_loss)\n",
    "        hist[\"val_loss\"].append(val_loss)\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        improved = (best_val - val_loss) > min_delta\n",
    "        if improved:\n",
    "            best_val = val_loss\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "            best_epoch = epoch\n",
    "            since_improved = 0\n",
    "\n",
    "            # save checkpoint with normalization stats\n",
    "            ckpt_path = os.path.join(ckpt_dir, f\"{run_name}_best.pt\")\n",
    "            if learning_regime == \"window\":\n",
    "                torch.save({\n",
    "                    \"model_state_dict\": best_state,\n",
    "                    \"epoch\": best_epoch,\n",
    "                    \"val_loss\": best_val,\n",
    "                    \"config\": {\n",
    "                        \"R\": model.R,\n",
    "                    },\n",
    "                    \"normalization\": {\n",
    "                        \"mean_feat\": mean_feat.cpu(),\n",
    "                        \"std_feat\":  std_feat.cpu(),\n",
    "                        \"E_mean\":    E_mean.cpu(),\n",
    "                        \"E_std\":     E_std.cpu(),\n",
    "                        \"N_grid\":    int(N_grid),\n",
    "                    }\n",
    "                }, ckpt_path)\n",
    "            elif learning_regime == \"gaussmixt\":\n",
    "                torch.save({\n",
    "                    \"model_state_dict\": best_state,\n",
    "                    \"epoch\": best_epoch,\n",
    "                    \"val_loss\": best_val,\n",
    "                    \"config\": {\n",
    "                        \"n_components\": model.kernel_conv.n_components,                    },\n",
    "                    \"normalization\": {\n",
    "                        \"mean_feat\": mean_feat.cpu(),\n",
    "                        \"std_feat\":  std_feat.cpu(),\n",
    "                        \"E_mean\":    E_mean.cpu(),\n",
    "                        \"E_std\":     E_std.cpu(),\n",
    "                        \"N_grid\":    int(N_grid),\n",
    "                    }\n",
    "                }, ckpt_path)\n",
    "            elif learning_regime == \"dct_rs_blind\":\n",
    "                torch.save({\n",
    "                    \"model_state_dict\": best_state,\n",
    "                    \"epoch\": best_epoch,\n",
    "                    \"val_loss\": best_val,\n",
    "                    \"config\": {\n",
    "                        \"learning_mode\": \"dct_rs_blind\",  \n",
    "                        \"zero_r_flag\": model.nonlocal_kernel.zero_r_flag                  }, \n",
    "                    \"normalization\": {\n",
    "                        \"mean_feat\": mean_feat.cpu(),\n",
    "                        \"std_feat\":  std_feat.cpu(),\n",
    "                        \"E_mean\":    E_mean.cpu(),\n",
    "                        \"E_std\":     E_std.cpu(),\n",
    "                        \"N_grid\":    int(N_grid),\n",
    "                    }\n",
    "                }, ckpt_path)\n",
    "            elif learning_regime == \"dct_exp_rs_mixture\":\n",
    "                    torch.save({\n",
    "                    \"model_state_dict\": best_state,\n",
    "                    \"epoch\": best_epoch,\n",
    "                    \"val_loss\": best_val,\n",
    "                    \"config\": {\n",
    "                        \"learning_mode\": \"dct_exp_rs_mixture\",  \n",
    "                        \"zero_r_flag\": model.nonlocal_kernel.zero_r_flag,\n",
    "                        \"n_components\": model.nonlocal_kernel.n_components,}, \n",
    "                    \"normalization\": {\n",
    "                        \"mean_feat\": mean_feat.cpu(),\n",
    "                        \"std_feat\":  std_feat.cpu(),\n",
    "                        \"E_mean\":    E_mean.cpu(),\n",
    "                        \"E_std\":     E_std.cpu(),\n",
    "                        \"N_grid\":    int(N_grid),\n",
    "                    }\n",
    "                }, ckpt_path)\n",
    "            elif learning_regime == \"dct_ms_blind\":\n",
    "                    torch.save({\n",
    "                    \"model_state_dict\": best_state,\n",
    "                    \"epoch\": best_epoch,\n",
    "                    \"val_loss\": best_val,\n",
    "                    \"config\": {\n",
    "                        \"learning_mode\": \"dct_ms_blind\",  \n",
    "                        \"range_ms\": model.nonlocal_kernel.range_ms,}, \n",
    "                    \"normalization\": {\n",
    "                        \"mean_feat\": mean_feat.cpu(),\n",
    "                        \"std_feat\":  std_feat.cpu(),\n",
    "                        \"E_mean\":    E_mean.cpu(),\n",
    "                        \"E_std\":     E_std.cpu(),\n",
    "                        \"N_grid\":    int(N_grid),\n",
    "                    }\n",
    "                }, ckpt_path)\n",
    "            elif learning_regime == \"hybrid\":\n",
    "                torch.save({\n",
    "                        \"model_state_dict\": best_state,\n",
    "                        \"epoch\": best_epoch,\n",
    "                        \"val_loss\": best_val,\n",
    "                        \"config\": {  \n",
    "                            \"R\": model.R,\n",
    "                            \"n_exp_components\": model.n_exp_components,}, \n",
    "                        \"normalization\": {\n",
    "                            \"mean_feat\": mean_feat.cpu(),\n",
    "                            \"std_feat\":  std_feat.cpu(),\n",
    "                            \"E_mean\":    E_mean.cpu(),\n",
    "                            \"E_std\":     E_std.cpu(),\n",
    "                            \"N_grid\":    int(N_grid),\n",
    "                        }\n",
    "                    }, ckpt_path)\n",
    "        \n",
    "        else:\n",
    "            since_improved += 1\n",
    "\n",
    "        if (epoch % 10) == 0 or epoch == 1:\n",
    "            print(f\"[{epoch:04d}] train={train_loss:.6f} | val={val_loss:.6f} \"\n",
    "                  f\"| best_val={best_val:.6f} (epoch {best_epoch})\")\n",
    "\n",
    "        if since_improved >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch} (best @ {best_epoch}).\")\n",
    "            break\n",
    "\n",
    "    # restore best\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    csv_path = os.path.join(ckpt_dir, f\"{run_name}_history.csv\")\n",
    "    try:\n",
    "        import csv\n",
    "        with open(csv_path, \"w\", newline=\"\") as f:\n",
    "            w = csv.writer(f)\n",
    "            w.writerow([\"epoch\", \"train_loss\", \"val_loss\"])\n",
    "            for i, (tr, va) in enumerate(zip(hist[\"train_loss\"], hist[\"val_loss\"]), start=1):\n",
    "                w.writerow([i, tr, va])\n",
    "    except Exception as e:\n",
    "        print(f\"[warn] could not write CSV: {e}\")\n",
    "\n",
    "    return hist, best_epoch\n",
    "\n",
    "\n",
    "ckpt_dir = \"LearningNonLocalKernel_wRSDCTKernel_checkpoints\"\n",
    "flag_train = True  # set to True to train models\n",
    "learning_regime = \"hybrid\"  # \"dct_rs_blind\", \"dct_exp_rs_mixture\", \"dct_ms_blind\", or \"hybrid\"\n",
    "R = 5  # for hybrid\n",
    "n_exp_components = 1  # for hybrid\n",
    "range_ms = 50  # for dct_ms_blind\n",
    "\n",
    "if flag_train:\n",
    "\n",
    "    torch.manual_seed(1234) # for reproducibility\n",
    "    if kernel_regime == \"exp\":\n",
    "            run_name = f\"rs_dct_kernel_\" + data_regime + '_' + kernel_regime + f\"_xi{xi}\" + learning_regime\n",
    "    elif kernel_regime == \"yukawa\":\n",
    "        run_name = f\"rs_dct_kernel_\" + data_regime + '_' + kernel_regime + f\"_lam{lam}\" + learning_regime\n",
    "    elif kernel_regime == \"power\":\n",
    "        run_name = f\"rs_dct_kernel_\" + data_regime + '_' + kernel_regime + f\"_alpha{alpha}\" + learning_regime\n",
    "     \n",
    "    if learning_regime == \"dct_exp_rs_mixture\":\n",
    "        model = DCTKernelEnergyNN(learning_mode='dct_exp_rs_mixture', n_components=1).to(device)\n",
    "    elif learning_regime == \"dct_rs_blind\":\n",
    "        model = DCTKernelEnergyNN(learning_mode='dct_rs_blind').to(device)\n",
    "    elif learning_regime == \"dct_ms_blind\":\n",
    "        model = DCTKernelEnergyNN(learning_mode='dct_ms_blind', range_ms=range_ms).to(device)\n",
    "    elif learning_regime == \"hybrid\":\n",
    "        model = HybridKernelEnergyNN(R=R, n_exp_components=n_exp_components).to(device)\n",
    "\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Reduce LR when val loss plateaus\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=patience, cooldown=2, min_lr=1e-6\n",
    "    )\n",
    "\n",
    "    hist, best_epoch = train_with_early_stopping(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        criterion,\n",
    "        scheduler=scheduler,\n",
    "        max_epochs=N_epochs,\n",
    "        patience=patience,\n",
    "        min_delta=min_delta,\n",
    "        ckpt_dir=ckpt_dir,\n",
    "        run_name=run_name,\n",
    "        learning_regime=learning_regime,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a33478",
   "metadata": {},
   "outputs": [],
   "source": [
    "if kernel_regime == \"exp\":\n",
    "            run_name = f\"rs_dct_kernel_\" + data_regime + '_' + kernel_regime + f\"_xi{xi}\" + learning_regime\n",
    "elif kernel_regime == \"yukawa\":\n",
    "    run_name = f\"rs_dct_kernel_\" + data_regime + '_' + kernel_regime + f\"_lam{lam}\" + learning_regime   \n",
    "elif kernel_regime == \"power\":\n",
    "    run_name = f\"rs_dct_kernel_\" + data_regime + '_' + kernel_regime + f\"_alpha{alpha}\" + learning_regime\n",
    "\n",
    "path = ckpt_dir + f\"/{run_name}_history.csv\"\n",
    "hist_df = pd.read_csv(path)\n",
    "print(hist_df.head())\n",
    "hist_df.plot(x=\"epoch\", y=[\"train_loss\", \"val_loss\"], logy=True, grid=True, title=run_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d695d3a",
   "metadata": {},
   "source": [
    "Evaluate performance of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3fc477",
   "metadata": {},
   "outputs": [],
   "source": [
    "if kernel_regime == \"exp\":\n",
    "            run_name = f\"rs_dct_kernel_\" + data_regime + '_' + kernel_regime + f\"_xi{xi}\" + learning_regime\n",
    "elif kernel_regime == \"yukawa\":\n",
    "    run_name = f\"rs_dct_kernel_\" + data_regime + '_' + kernel_regime + f\"_lam{lam}\" + learning_regime\n",
    "elif kernel_regime == \"power\":\n",
    "    run_name = f\"rs_dct_kernel_\" + data_regime + '_' + kernel_regime + f\"_alpha{alpha}\" + learning_regime\n",
    "\n",
    "if learning_regime == \"hybrid\":\n",
    "    model, normalization, epoch, val_loss = load_checkpoint(\n",
    "        ckpt_dir + f\"/{run_name}_best.pt\",\n",
    "        HybridKernelEnergyNN,\n",
    "        device=device\n",
    "    )\n",
    "else:\n",
    "    model, normalization, epoch, val_loss = load_checkpoint(\n",
    "        ckpt_dir + f\"/{run_name}_best.pt\",\n",
    "        DCTKernelEnergyNN,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "if learning_regime == \"dct_exp_rs_mixture\":\n",
    "     print(\"Learned Exponential Mixture RS DCT Kernel parameters:\")\n",
    "     print(model.nonlocal_kernel.n_components)\n",
    "     print(\"Amplitudes A_n:\", model.nonlocal_kernel.amplitudes.detach().cpu().numpy())\n",
    "     print(\"Sigmas sigma_n:\", F.softplus(model.nonlocal_kernel.log_sigmas).detach().cpu().numpy())\n",
    "     with torch.no_grad():\n",
    "        k_full = model.nonlocal_kernel.build_kernel().detach().cpu().numpy()  # (N_grid,)\n",
    "elif learning_regime == \"dct_rs_blind\":\n",
    "    with torch.no_grad():\n",
    "        k_full = model.nonlocal_kernel.rs_kernel.detach().cpu().numpy()  # (N_grid,)\n",
    "elif learning_regime == \"dct_ms_blind\":\n",
    "    print(\"Learned MS DCT Kernel parameters:\")\n",
    "    print(\"Range m:\", model.nonlocal_kernel.range_ms)\n",
    "    with torch.no_grad():\n",
    "        lam_K = torch.zeros(N_grid, dtype=dtype)\n",
    "        lam_K[0] = 0.0\n",
    "        lam_K[1:1+range_ms] = model.nonlocal_kernel.ms_kernel.detach().cpu()\n",
    "        k_full = kernel_from_eigenvals_dct(lam_K).cpu().numpy()\n",
    "elif learning_regime == \"hybrid\":\n",
    "    print(\"Learned Hybrid Kernel parameters:\")\n",
    "    print(\"Local kernel range R:\", model.R)\n",
    "    print(\"Nonlocal exponential mixture components:\", model.n_exp_components)\n",
    "\n",
    "    print(\"Amplitudes A_n:\", model.nonlocal_kernel.amplitudes.detach().cpu().numpy())\n",
    "    print(\"Sigmas sigma_n:\", F.softplus(model.nonlocal_kernel.log_sigmas).detach().cpu().numpy())\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        k_local_small = model.local_kernel.build_kernel().squeeze()  # (2R+1,)\n",
    "        k_nonlocal = model.nonlocal_kernel.build_kernel()            # (N_grid,)\n",
    "\n",
    "        R = model.R  # local range\n",
    "        print(\"Local kernel values K_r (r=0..R):\", k_local_small[R:].cpu().numpy())\n",
    "\n",
    "        # take r = 0..R from symmetric kernel and pad zeros up to N_grid\n",
    "        # k_local_small layout: [-R, ..., -1, 0, 1, ..., R]\n",
    "        # index R is r=0, R+1..2R is r=1..R\n",
    "        k_local_full = torch.cat(\n",
    "            [\n",
    "                k_local_small[R:],  # (R+1,)  -> r=0..R\n",
    "                torch.zeros(N_grid - 1 - R, device=device, dtype=dtype),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )  # (N_grid,)\n",
    "        k_full = k_local_full.cpu().numpy() + k_nonlocal.cpu().numpy()  # (N_grid,)\n",
    "\n",
    "\n",
    "r_grid = np.arange(0, N_grid)\n",
    "\n",
    "if kernel_regime == \"exp\":\n",
    "    def K_true(r):\n",
    "        return np.exp(-np.abs(r) / xi)\n",
    "elif kernel_regime == \"yukawa\":\n",
    "    def K_true(r):\n",
    "        r_abs = np.abs(r)\n",
    "        out = np.exp(-r_abs / lam) / np.maximum(r_abs, 1.0)\n",
    "        return out * (r_abs > 0)\n",
    "elif kernel_regime == \"power\":\n",
    "    def K_true(r):\n",
    "        r_abs = np.abs(r)\n",
    "        out = 1.0 / (np.maximum(r_abs, 1.0)**alpha)\n",
    "        return out * (r_abs > 0)    \n",
    "\n",
    "k_true = K_true(r_grid)\n",
    "lamb_K_true = kernel_eigenvals_dct(torch.tensor(k_true, dtype=dtype))\n",
    "lamb_K_true[0] = 0.0  # enforce zero mode\n",
    "k_true = kernel_from_eigenvals_dct(lamb_K_true).cpu().numpy()\n",
    "\n",
    "lamb_K_full = kernel_eigenvals_dct(torch.tensor(k_full, dtype=dtype))\n",
    "lamb_K_full[0] = 0.0  # enforce zero mode\n",
    "k_full = kernel_from_eigenvals_dct(lamb_K_full).cpu().numpy()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(r_grid, k_full , 'o-', label='Learned kernel (window)', linewidth=2)\n",
    "plt.plot(r_grid, k_true, 's--', label='True kernel', alpha=0.7)\n",
    "plt.axhline(0, color='k', linewidth=0.5)\n",
    "plt.xlabel(\"r = i - j\")\n",
    "plt.ylabel(\"K_r\")\n",
    "plt.xlim(0, 100)\n",
    "# plt.ylim((-0.01, 0.01))\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "if kernel_regime == \"exp\":\n",
    "    plt.title(f\"Local kernel learning ({data_regime}, exp, xi={xi})\")\n",
    "elif kernel_regime == \"yukawa\":\n",
    "    plt.title(f\"Local kernel learning ({data_regime}, yukawa, lam={lam})\")\n",
    "elif kernel_regime == \"power\":\n",
    "    plt.title(f\"Local kernel learning ({data_regime}, power, alpha={alpha})\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(r_grid, lamb_K_full , 'o-', label='Learned kernel (window)', linewidth=2)\n",
    "plt.plot(r_grid, lamb_K_true, 's--', label='True kernel', alpha=0.7)\n",
    "plt.axhline(0, color='k', linewidth=0.5)\n",
    "plt.xlabel(\"m\")\n",
    "plt.ylabel(\"lambda_m\")\n",
    "plt.xlim(0, 100)\n",
    "#plt.ylim((-0.05, 0.0))\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "if kernel_regime == \"exp\":\n",
    "    plt.title(f\"Local kernel learning ({data_regime}, exp, xi={xi})\")\n",
    "elif kernel_regime == \"yukawa\":\n",
    "    plt.title(f\"Local kernel learning ({data_regime}, yukawa, lam={lam})\")\n",
    "elif kernel_regime == \"power\":\n",
    "    plt.title(f\"Local kernel learning ({data_regime}, power, alpha={alpha})\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a0a992",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
